{"searchDocs":[{"title":"Introduction","type":0,"sectionRef":"#","url":"/docs/","content":"Introduction Good APIs craft a broad spectrum of functionalities. Yet, the broader their scope, the more they diverge from being the perfect fit for any specific use case. This fundamental discrepancy — the impedance mismatch between the general capabilities of an API and the precise needs of a particular scenario — amplifies the necessity for an orchestration layer. Such a layer adeptly bridges this gap, tailor-fitting generic APIs to meet exact requirements with finesse. Tailcall stands at the forefront of this innovation, seamlessly transforming the way APIs are integrated and interacted with. Tailcall introduces a robust DSL (Domain-Specific Language), enabling developers to fine-tune how APIs are orchestrated. This DSL facilitates specifying different caching and batching strategies to enhance the system's efficiency. It also enables precise governance and access control mechanisms. Tailcall serves as a central hub for team collaboration, offering a unified point for managing all APIs, documentation, and more. Once configured, it positions itself between the clients and microservices, adeptly managing all requests and orchestrating them as needed. Manually crafting BFF (Backend for Frontend) layers has become outdated. With Tailcall, API orchestration evolves into a streamlined and highly optimized process. It functions as an essential intermediary, intelligently directing requests and assembling responses from each microservice. This approach diminishes the development burden associated with traditional BFF layers but also bolsters performance, reliability, and scalability throughout the application infrastructure.","keywords":"","version":"Next"},{"title":"Basic Auth","type":0,"sectionRef":"#","url":"/docs/auth/basic-auth/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Basic Auth","url":"/docs/auth/basic-auth/#prerequisites","content":" To be able to use Basic Authentication support you should have configured htpasswd file that contains users credentials data.  To generate this data you can use Apache tooling itself or available web-tool  important Since this file stores secure information make sure to hash the password you use with secure algorithms  ","version":"Next","tagName":"h2"},{"title":"Tailcall config​","type":1,"pageTitle":"Basic Auth","url":"/docs/auth/basic-auth/#tailcall-config","content":" To use Basic Auth you should first include htpasswd file generated from Prerequisites with the help of @link directive.  We can use that file as an example for it that has data for testuser:mypassword credentials in encrypted format:  htpasswd testuser:$2y$10$wJ/mZDURcAOBIrswCAKFsO0Nk7BpHmWl/XuhF7lNm3gBAFH3ofsuu   After adding @link you can use the @protected directive to mark the fields that requiring success authentication to be requested.  The whole example could look like this:  schema @server(port: 8000, graphiql: true) @upstream(baseURL: &quot;http://jsonplaceholder.typicode.com&quot;) @link(id: &quot;auth-basic&quot;, type: Htpasswd, src: &quot;htpasswd&quot;) { query: Query } type Query { user(id: Int!): User @http(path: &quot;/users/{{args.id}}&quot;) } type User @protected { id: Int! name: String! username: String! email: String! phone: String website: String }   ","version":"Next","tagName":"h2"},{"title":"Making test request​","type":1,"pageTitle":"Basic Auth","url":"/docs/auth/basic-auth/#making-test-request","content":" Now you can run the example file with Tailcall and try to make a query for data with specifying credentials.  To make the request first create base64 encoded string from the testuser:mypassword string and then append the result to the Authorization: Basic header.  A request example with curl:  curl --request POST \\ --url http://localhost:8000/graphql \\ --header 'Authorization: Basic dGVzdHVzZXI6bXlwYXNzd29yZA==' \\ --data '{&quot;query&quot;:&quot;query {\\n\\tuser(id: 1) { name }\\n}&quot;}'   or you can use the GraphQL Playground and add the header in the HTTP Headers section:  { &quot;Authorization&quot;: &quot;Basic dGVzdHVzZXIyOm15cGFzc3dvcmQ=&quot; }   with query:  query { user(id: 1) { name } }   Executing such request should be resolved with the user and its name. ","version":"Next","tagName":"h2"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/docs/directives/","content":"Introduction Tailcall DSL builds on your existing GraphQL knowledge by allowing the addition of some custom directives. These directives provide powerful compile time guarantees to ensure your API composition is tight and robust. Tailcall automatically generates highly optimized resolver logic for your types using the information in the directives. Here is a list of all the custom directives supported by Tailcall: Certainly! Here's the table with hyperlinks added back to the directive names: Operator\tDescription@addField\tSimplifies data structures and queries by adding, inlining, or flattening fields or nodes within the schema. @cache\tEnables caching for the query, field or type applied to. @call\tInvokes a query or mutation from another query or mutation field. @expr\tAllows embedding of a constant response within the schema. @graphQL\tResolves a field or node by a GraphQL API. @grpc\tResolves a field or node by a gRPC API. @http\tResolves a field or node by a REST API. @link\tImports external resources such as config files, certs, protobufs, etc in the schema. @modify\tEnables changes to attributes of fields or nodes in the schema. @omit\tExcludes fields or nodes from the generated schema, making them inaccessible through the GraphQL API. @rest\tAllows exposing REST endpoints on top of GraphQL. @server\tProvides server configurations for behavior tuning and tailcall optimization in specific use-cases. @telemetry\tIntegrates with open-telemetry to provide observability of the running tailcall service. @upstream\tControls aspects of the upstream server connection, including timeouts and keep-alive settings.","keywords":"","version":"Next"},{"title":"JWT","type":0,"sectionRef":"#","url":"/docs/auth/jwt/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"JWT","url":"/docs/auth/jwt/#prerequisites","content":" To be able to use JWT authentication you should have configured JSON Web Key Sets (JWKS for short) file.  To create this file you can use available web-tools like JWK creator in case you already have RSA key-pair or mkjwk if you don't.  ","version":"Next","tagName":"h2"},{"title":"Tailcall config​","type":1,"pageTitle":"JWT","url":"/docs/auth/jwt/#tailcall-config","content":" To use JWT you should first include JWKS file generated from Prerequisites with the help of @link directive.  We can use that file as an example for it:  jwks.json { &quot;keys&quot;: [ { &quot;kty&quot;: &quot;RSA&quot;, &quot;use&quot;: &quot;sig&quot;, &quot;alg&quot;: &quot;RS256&quot;, &quot;kid&quot;: &quot;I48qMJp566SSKQogYXYtHBo9q6ZcEKHixNPeNoxV1c8&quot;, &quot;n&quot;: &quot;ksMb5oMlhJ_HzAebCuBG6-v5Qc4J111ur7Aux6-8SbxzqFONsf2Bw6ATG8pAfNeZ-USA3_T1mGkYTDvfoggXnxsduWV_lePZKKOq_Qp_EDdzic1bVTJQDad3CXldR3wV6UFDtMx6cCLXxPZM5n76e7ybPt0iNgwoGpJE28emMZJXrnEUFzxwFMq61UlzWEumYqW3uOUVp7r5XAF5jQ_1nQAnpHBnRFzdNPVb3E6odMGu3jgp8mkPbPMP16Fund4LVplLz8yrsE9TdVrSdYJThylRWn_BwvJ0DjUcp8ibJya86iClUlixAmBwR9NdStHwQqHwmMXMKkTXo-ytRmSUobzxX9T8ESkij6iBhQpmDMD3FbkK30Y7pUVEBBOyDfNcWOhholjOj9CRrxu9to5rc2wvufe24VlbKb9wngS_uGfK4AYvVyrcjdYMFkdqw-Mft14HwzdO2BTS0TeMDZuLmYhj_bu5_g2Zu6PH5OpIXF6Fi8_679pCG8wWAcFQrFrM0eA70wD_SqD_BXn6pWRpFXlcRy_7PWTZ3QmC7ycQFR6Wc6Px44y1xDUoq3rH0RlZkeicfvP6FRlpjFU7xF6LjAfd9ciYBZfJll6PE7zf-i_ZXEslv-tJ5-30-I4Slwj0tDrZ2Z54OgAg07AIwAiI5o4y-0vmuhUscNpfZsGAGhE&quot;, &quot;e&quot;: &quot;AQAB&quot; } ] }   After adding @link you can use the @protected directive to mark the fields that requiring success authentication to be requested.  The whole example could look like this:  schema @server(port: 8000, graphiql: true) @upstream(baseURL: &quot;http://jsonplaceholder.typicode.com&quot;) @link(id: &quot;auth-jwks&quot;, type: Jwks, src: &quot;jwks.json&quot;) { query: Query } type Query { user(id: Int!): User @http(path: &quot;/users/{{args.id}}&quot;) } type User @protected { id: Int! name: String! username: String! email: String! phone: String website: String }   ","version":"Next","tagName":"h2"},{"title":"Making test request​","type":1,"pageTitle":"JWT","url":"/docs/auth/jwt/#making-test-request","content":" Now you can run the example file with Tailcall and try to make a query for data with specifying credentials.  To make the request first obtain JWT token compatible with JWKS file you've linked before (if you've used the example jwks.json file from above then you can use the token from the example below).  An request example with curl:  curl --request POST \\ --url http://localhost:8000/graphql \\ --header 'Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6Ikk0OHFNSnA1NjZTU0tRb2dZWFl0SEJvOXE2WmNFS0hpeE5QZU5veFYxYzgifQ.eyJleHAiOjIwMTkwNTY0NDEuMCwiaXNzIjoibWUiLCJzdWIiOiJ5b3UiLCJhdWQiOlsidGhlbSJdfQ.cU-hJgVGWxK3-IBggYBChhf3FzibBKjuDLtq2urJ99FVXIGZls0VMXjyNW7yHhLLuif_9t2N5UIUIq-hwXVv7rrGRPCGrlqKU0jsUH251Spy7_ppG5_B2LsG3cBJcwkD4AVz8qjT3AaE_vYZ4WnH-CQ-F5Vm7wiYZgbdyU8xgKoH85KAxaCdJJlYOi8mApE9_zcdmTNJrTNd9sp7PX3lXSUu9AWlrZkyO-HhVbXFunVtfduDuTeVXxP8iw1wt6171CFbPmQJU_b3xCornzyFKmhSc36yvlDfoPPclWmWeyOfFEp9lVhQm0WhfDK7GiuRtaOxD-tOvpTjpcoZBeJb7bSg2OsneyeM_33a0WoPmjHw8WIxbroJz_PrfE72_TzbcTSDttKAv_e75PE48Vvx0661miFv4Gq8RBzMl2G3pQMEVCOm83v7BpodfN_YVJcqZJjVHMA70TZQ4K3L4_i9sIK9jJFfwEDVM7nsDnUu96n4vKs1fVvAuieCIPAJrfNOUMy7TwLvhnhUARsKnzmtNNrJuDhhBx-X93AHcG3micXgnqkFdKn6-ZUZ63I2KEdmjwKmLTRrv4n4eZKrRN-OrHPI4gLxJUhmyPAHzZrikMVBcDYfALqyki5SeKkwd4v0JAm87QzR4YwMdKErr0Xa5JrZqHGe2TZgVO4hIc-KrPw' \\ --data '{&quot;query&quot;:&quot;query {\\n\\tuser(id: 1) { name }\\n}&quot;}'   Executing such request should be resolved with the user and its name. ","version":"Next","tagName":"h2"},{"title":"@cache","type":0,"sectionRef":"#","url":"/docs/directives/cache/","content":"","keywords":"","version":"Next"},{"title":"maxAge​","type":1,"pageTitle":"@cache","url":"/docs/directives/cache/#maxage","content":" @cache(maxAge: Int)   This parameter is a non-zero unsigned integer specifying the duration, in milliseconds, that retains the cached value.  ","version":"Next","tagName":"h2"},{"title":"Usage​","type":1,"pageTitle":"@cache","url":"/docs/directives/cache/#usage","content":" Consider the following GraphQL schema example:  type Query { posts: [Post] @http(path: &quot;/posts&quot;) } type Post { id: Int title: String userId: Int @cache(maxAge: 100) user: User @http(path: &quot;/user/{{value.userId}}&quot;) @cache(maxAge: 200) } type User { id: Int name: String email: String }   In this configuration, the system caches the result of the user field due to its association with an HTTP resolver. But it does not cache the values of userId and title because they lack individual resolvers; the resolver for the posts field retrieves their values, employing the @http(path: &quot;/posts&quot;) directive.  Applying the @cache directive at the type level affects all fields within that type. For example:  type Query { posts: [Post] @http(path: &quot;/posts&quot;) } type Post @cache(maxAge: 100) { id: Int title: String userId: Int user: User @http(path: &quot;/user/{{value.userId}}&quot;) } type User { id: Int name: String email: String }   You can simplify this configuration to show that applying the @cache directive to a type means every field within that type inherits it:  type Query { posts: [Post] @http(path: &quot;/posts&quot;) } type Post { id: Int @cache(maxAge: 100) title: String @cache(maxAge: 100) userId: Int @cache(maxAge: 100) user: User @http(path: &quot;/user/{{value.userId}}&quot;) @cache(maxAge: 100) } type User { id: Int name: String email: String }   Since the @cache directive does not affect fields without resolvers, the effective configuration can be further reduced as follows:  type Query { posts: [Post] @http(path: &quot;/posts&quot;) } type Post { id: Int title: String userId: Int user: User @http(path: &quot;/user/{{value.userId}}&quot;) @cache(maxAge: 100) } type User { id: Int name: String email: String }   When applying the @cache directive both at the type level and on individual fields within that type, the field-level directive takes precedence:  type Query { posts: [Post] @http(path: &quot;/posts&quot;) } type Post @cache(maxAge: 200) { id: Int title: String userId: Int user: User @http(path: &quot;/user/{{value.userId}}&quot;) @cache(maxAge: 100) } type User { id: Int name: String email: String }   Thus, in the configuration above, while all fields inherit the @cache(maxAge: 200) directive at the type level, the user field's explicit @cache(maxAge: 100) directive takes precedence.  ","version":"Next","tagName":"h2"},{"title":"Cache Key​","type":1,"pageTitle":"@cache","url":"/docs/directives/cache/#cache-key","content":" The caching mechanism generates a hash based on information related to the applied query to serve as the cache key for the corresponding value.  For instance, the system caches the user field in the following configuration, using the hash of the interpolated string &quot;/user/{{value.userId}}&quot; as the cache key. For example, if Post.userId equals 1, the system generates the cache key by hashing the string &quot;/users/1&quot;. ","version":"Next","tagName":"h2"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/docs/auth/","content":"","keywords":"","version":"Next"},{"title":"What is Authentication?​","type":1,"pageTitle":"Introduction","url":"/docs/auth/#what-is-authentication","content":" Authentication is the process of verifying a user's identity before granting access to data. In most modern applications, some information, such as a list of products in an e-commerce application, is accessible to all users without requiring identification. However, personal data, like a user's order history, is accessible to the user who owns that information. Verifying a user's identity to access such personal data is known as authentication.  The primary reasons for implementing authentication in an application include:  Protecting User-Specific Data Ensuring that data belonging to a user is not accessible by others.Security The ability to block users based on certain criteria necessitates identifying them.Customized User Experiences Delivering personalized experiences based on a user's identity.  Authentication can be implemented using credential validation mechanisms, such as:  Basic AuthJWTOAuthAPI Key  ","version":"Next","tagName":"h2"},{"title":"Entity Level Authentication in GraphqQL​","type":1,"pageTitle":"Introduction","url":"/docs/auth/#entity-level-authentication-in-graphqql","content":" Entity level authentication in GraphQL refers to applying authentication logic to specific entities or types within your GraphQL schema, rather than at the API entry point or resolver level for individual queries or mutations. This approach allows you to control access to particular data types or fields based on the user's authentication status, enabling a more granular and flexible security model.  Advantages of this approach:  Flexibility: Tailors security measures to precisely fit the needs of your application, enhancing the protection of sensitive data.Scalability: Facilitates extending security policies to new entities and fields as your schema expands.Customization: Enables implementing different authentication mechanisms for distinct entities based on their security requirements.  ","version":"Next","tagName":"h3"},{"title":"Tailcall Authentication​","type":1,"pageTitle":"Introduction","url":"/docs/auth/#tailcall-authentication","content":" Tailcall provides a straightforward way to implement entity level authentication in your GraphQL schema. By leveraging custom directives, you can define which entities or fields require authentication to access their data. Tailcall supports multiple authentication providers, such as Basic Auth and JWT, allowing you to choose the authentication mechanism that best suits your application's requirements. to know more about how to use it, read the following articles:  Basic AuthJWT  ","version":"Next","tagName":"h2"},{"title":"Tailcall config​","type":1,"pageTitle":"Introduction","url":"/docs/auth/#tailcall-config","content":" Enabling support for authentication in Tailcall could be done in two steps:  With the help of @link directive connect multiple authentication files as you need for different provides. To connect it use either Htpasswd or Jwks link typeMark that some type of field requires authentication to be fetched with the help of @protected directive  Your config could look like this now:  schema @server(port: 8000, graphiql: true) @upstream(baseURL: &quot;http://jsonplaceholder.typicode.com&quot;) @link(id: &quot;auth-basic&quot;, type: Htpasswd, src: &quot;htpasswd&quot;) @link(id: &quot;auth-jwt&quot;, type: Jwks, src: &quot;jwks.json&quot;) { query: Query mutation: Mutation } type Query { posts: [Post] @http(path: &quot;/posts&quot;) user(id: Int!): User @http(path: &quot;/users/{{args.id}}&quot;) } type Mutation { user(id: Int!): User @http(path: &quot;/users/{{args.id}}&quot;) } type User @protected { id: Int! name: String! username: String! email: String! phone: String website: String } type Post { id: Int! userId: Int! title: String! body: String! @protected user: User @http(path: &quot;/users/{{value.userId}}&quot;) }   In that case the whole User type and Post.body are marked as protected and therefore requiring authentication to resolve its content. That means following points:  any query for Post.body will require authenticationany query for any field of User will require authenticationany field that resolves to User type will require authentication  For more info about possible configuration for available providers read articles for Basic Auth and JWT  ","version":"Next","tagName":"h2"},{"title":"Making test requests​","type":1,"pageTitle":"Introduction","url":"/docs/auth/#making-test-requests","content":" Now let's try to access some data from the example above. Start the tailcall server with provided config and use graphQL playground that should be opened automatically in your browser.  If you execute the query that don't have any @protected fields like  { posts { title } }   Then the data for this will be resolved as usual without providing any additional info. showing the list of posts with their titles:But if you change the query to access protected data, then if you don't provide any authentication data, i.e. for query:  { posts { body } }   You will get an authentication failure error stating that authentication parameters were not provided. e.g.:  { &quot;data&quot;: null, &quot;errors&quot;: [ { &quot;message&quot;: &quot;Authentication Failure: Parameters not provided in the request.&quot;, &quot;locations&quot;: [ { &quot;line&quot;: 3, &quot;column&quot;: 5 } ] } ] }     Now update the request by providing additional Authorization header. You can do in the Playground by navigating to the tab HTTP HEADERS at the bottom by adding following header for Basic Auth:  { &quot;Authorization&quot;: &quot;Basic dGVzdHVzZXIxOnBhc3N3b3JkMTIzs&quot; }   Now after executing the request again you'll get the response for all the requested fields without any error.  ","version":"Next","tagName":"h2"},{"title":"How it works​","type":1,"pageTitle":"Introduction","url":"/docs/auth/#how-it-works","content":" ","version":"Next","tagName":"h2"},{"title":"@protected Type​","type":1,"pageTitle":"Introduction","url":"/docs/auth/#protected-type","content":" If type is marked with @protected then:  attempt to request any field of that type will require authenticationattempt to request any field from other type that resolves to protected type will require authentication and the underlying IO operation won't be executed without it  ","version":"Next","tagName":"h3"},{"title":"Mutation​","type":1,"pageTitle":"Introduction","url":"/docs/auth/#mutation","content":" For mutation entity level authentication works similar to queries. But since mutation involves requests that changes external state you should be careful where do you specify @protected directive because marking some nested field as protected doesn't prevent from executing the request to resolve the parent fields. I.e. following example is problematic:  schema { query: Query mutation: Mutation } type Query { user(id: Int!): User @http(path: &quot;/users/{{args.id}}&quot;) } type Mutation { user(id: Int!): User @http(path: &quot;/users/{{args.id}}&quot;, method: POST) } type User { id: Int! name: String! website: String @protected }   Here you can still execute the mutation without any authentication and fail on attempting to resolve website field.  To resolve this issue, consider marking root fields as protected in case they require authentication, i.e.:  schema { query: Query mutation: Mutation } type Query { user(id: Int!): User @http(path: &quot;/users/{{args.id}}&quot;) } type Mutation { user(id: Int!): User @http(path: &quot;/users/{{args.id}}&quot;, method: POST) @protected } type User { id: Int! name: String! website: String @protected }   ","version":"Next","tagName":"h3"},{"title":"Multiple auth providers​","type":1,"pageTitle":"Introduction","url":"/docs/auth/#multiple-auth-providers","content":" In case you linked multiple authentication files all of them will be used to execute validation process. In that case, by default, Tailcall will validate all of them in parallel and succeed if at least one of them succeed.  ","version":"Next","tagName":"h3"},{"title":"Authentication headers​","type":1,"pageTitle":"Introduction","url":"/docs/auth/#authentication-headers","content":" To validate authentication for user request the specific headers are used (like Authorization header). In case auth is enabled for tailcall those headers will be also added to the allowedHeaders list and therefore they will be forwarded to the upstream requests implicitly. ","version":"Next","tagName":"h3"},{"title":"@addField","type":0,"sectionRef":"#","url":"/docs/directives/add-field/","content":"@addField The @addField directive simplifies data structures and queries by adding a field that inline or flattens a nested field or node within your schema. It modifies the schema and the data transformation process, making nested data more accessible and straightforward to present. For instance, consider a schema: schema { query: Query } type User @addField(name: &quot;street&quot;, path: [&quot;address&quot;, &quot;street&quot;]) { id: Int! name: String! username: String! email: String! phone: String website: String address: Address @modify(omit: true) } type Address { street: String! city: String! state: String! } type Query { user(id: Int!): User @http(path: &quot;/users/{{args.id}}&quot;) } Suppose we focus on the street field in Address. In this case, applying the @addField directive to the User type creates a street field within the User type. It uses a path argument to specify the sequence of fields from a declared field (address), leading to the Address field to add. We also can apply @modify(omit: true) to remove the address field from the schema, as the street field from Address is now directly accessible on the User type. Post application, the schema becomes: schema { query: Query } type User { id: Int! name: String! username: String! email: String! phone: String website: String street: String } type Query { user(id: Int): Post! } In the above example, since we added a @modify(omit: true) on the address field, the schema no longer includes the Address type. The @addField directive also take cares of nullablity of the fields. If any of the fields in the path is nullable, the resulting type will be nullable. @addField also supports indexing, allowing for the specification of an array index for inline inclusion. For instance, if a field posts is of type [Post], and the goal is to access the title of the first post, specify the path as [&quot;posts&quot;,&quot;0&quot;,&quot;title&quot;]. type User @addField( name: &quot;firstPostTitle&quot; path: [&quot;posts&quot;, &quot;0&quot;, &quot;title&quot;] ) { id: Int! name: String! username: String! email: String! phone: String website: String posts: Post @http(path: &quot;/users/{{value.id}}/posts&quot;) } type Post { id: Int! userId: Int! title: String! body: String! } In conclusion, the @addField directive helps tidy up your schema and streamline data fetching by reducing query depth, promoting better performance and simplicity.","keywords":"","version":"Next"},{"title":"@expr","type":0,"sectionRef":"#","url":"/docs/directives/expr/","content":"","keywords":"","version":"Next"},{"title":"Static​","type":1,"pageTitle":"@expr","url":"/docs/directives/expr/#static","content":" This feature allows for the inclusion of a constant response within the schema definition itself. It is useful for scenarios where the response is unchanging. e.g:  schema { query: Query } type Query { user: User @expr(body: {name: &quot;John&quot;, age: 12}) } type User { name: String age: Int }   The @expr directive also checks the provided value at compile time to ensure it matches the field's schema. If not, the console displays a descriptive error message.  ","version":"Next","tagName":"h2"},{"title":"Dynamic​","type":1,"pageTitle":"@expr","url":"/docs/directives/expr/#dynamic","content":" Beyond static data embedding, the @expr directive extends its utility to support dynamic data injection through Mustache template syntax. This feature enables the use of placeholders within the constant data, which are then dynamically replaced with actual values at runtime. It supports both scalar values and complex objects, including lists and nested objects, offering flexibility in tailoring responses to specific needs. e.g:  schema { query: Query } type Query { user: User @expr( body: { name: &quot;John&quot; workEmail: &quot;john@xyz.com&quot; personalEmail: &quot;john@xyz.com&quot; } ) } type User { name: String age: Int personalEmail: String workEmail: String emails: Emails @expr( body: { emails: { workEmail: &quot;{{value.workEmail}}&quot; personalEmail: &quot;{{value.personalEmail}}&quot; } } ) } type Emails { workEmail: String personalEmail: String }   In this example, the @expr directive dynamically generate an Emails object based on the provided template data. The placeholders within the template ({{value.workEmail}} and {{value.personalEmail}}) gets replaced with the actual values specified in the User type, allowing for dynamic content generation while still adhering to the schema's structure. ","version":"Next","tagName":"h2"},{"title":"@grpc","type":0,"sectionRef":"#","url":"/docs/directives/grpc/","content":"","keywords":"","version":"Next"},{"title":"method​","type":1,"pageTitle":"@grpc","url":"/docs/directives/grpc/#method","content":" This parameter specifies the gRPC service and method to be invoked, formatted as &lt;package&gt;.&lt;service&gt;.&lt;method&gt;:  type Query { users: [User] @grpc(method: &quot;proto.users.UserService.ListUsers&quot;) }   ","version":"Next","tagName":"h2"},{"title":"baseURL​","type":1,"pageTitle":"@grpc","url":"/docs/directives/grpc/#baseurl","content":" Defines the base URL for the gRPC API. If not specified, the URL set in the @upstream directive is used by default:  type Query { users: [User] @grpc( baseURL: &quot;https://grpc-server.example.com&quot; method: &quot;proto.users.UserService.ListUsers&quot; ) }   ","version":"Next","tagName":"h2"},{"title":"body​","type":1,"pageTitle":"@grpc","url":"/docs/directives/grpc/#body","content":" This parameter outlines the arguments for the gRPC call, allowing for both static and dynamic inputs:  type UserInput { id: ID } type Query { user(id: UserInput!): User @grpc( body: &quot;{{args.id}}&quot; method: &quot;proto.users.UserService.GetUser&quot; ) }   ","version":"Next","tagName":"h2"},{"title":"headers​","type":1,"pageTitle":"@grpc","url":"/docs/directives/grpc/#headers","content":" Custom headers for the gRPC request can be defined, facilitating the transmission of authentication tokens or other contextual data:  type Query { users: [User] @grpc( headers: [ {key: &quot;X-CUSTOM-HEADER&quot;, value: &quot;custom-value&quot;} ] method: &quot;proto.users.UserService.ListUsers&quot; ) }   ","version":"Next","tagName":"h2"},{"title":"batchKey​","type":1,"pageTitle":"@grpc","url":"/docs/directives/grpc/#batchkey","content":" This argument is employed to optimize batch requests by grouping them based on specified response keys, enhancing performance in scenarios requiring multiple, similar requests:  type Query { users(id: UserInput!): [User] @grpc( batchKey: [&quot;id&quot;] method: &quot;proto.users.UserService.ListUsers&quot; baseURL: &quot;https://grpc-server.example.com&quot; ) }   info Read about n + 1 to learn how to use the batchKey setting. ","version":"Next","tagName":"h2"},{"title":"@graphQL","type":0,"sectionRef":"#","url":"/docs/directives/graphql/","content":"","keywords":"","version":"Next"},{"title":"baseURL​","type":1,"pageTitle":"@graphQL","url":"/docs/directives/graphql/#baseurl","content":" This refers to the base URL of the API. If not specified, the default base URL is the one specified in the @upstream directive.  type Query { users: [User] @graphQL( name: &quot;users&quot; baseURL: &quot;https://graphqlzero.almansi.me/api&quot; ) }   ","version":"Next","tagName":"h3"},{"title":"name​","type":1,"pageTitle":"@graphQL","url":"/docs/directives/graphql/#name","content":" The root field's name on the upstream to request data from. For example:  type Query { users: [User] @graphQL(name: &quot;userList&quot;) }   When Tailcall receives a query for the users field, it will request a query for userList from the upstream.  ","version":"Next","tagName":"h3"},{"title":"args​","type":1,"pageTitle":"@graphQL","url":"/docs/directives/graphql/#args","content":" Named arguments for the requested field. For example:  type Query { user: User @graphQL( name: &quot;user&quot; args: [{key: &quot;id&quot;, value: &quot;{{value.userId}}&quot;}] ) }   Will request the next query from the upstream for the first user's name:  query { user(id: 1) { name } }   ","version":"Next","tagName":"h3"},{"title":"headers​","type":1,"pageTitle":"@graphQL","url":"/docs/directives/graphql/#headers","content":" The headers parameter allows customizing the headers of the GraphQL request made by the @graphQL directive. Specifying a key-value map of header names and their values achieves this.  For instance:  type Mutation { users: User @graphQL( name: &quot;users&quot; headers: [{key: &quot;X-Server&quot;, value: &quot;Tailcall&quot;}] ) }   In this example, a request to /users will include the HTTP header X-Server with the value Tailcall.  ","version":"Next","tagName":"h3"},{"title":"batch​","type":1,"pageTitle":"@graphQL","url":"/docs/directives/graphql/#batch","content":" In case the upstream GraphQL server supports request batching, we can specify the batch argument to batch requests to a single upstream into a single batch request. For example:  schema @upstream( batch: { maxSize: 1000 delay: 10 headers: [&quot;X-Server&quot;, &quot;Authorization&quot;] } ) { query: Query mutation: Mutation } type Query { users: [User] @graphQL(name: &quot;users&quot;, batch: true) posts: [Post] @graphQL(name: &quot;posts&quot;, batch: true) }   Make sure you have also specified batch settings to the @upstream and to the @graphQL directive. ","version":"Next","tagName":"h3"},{"title":"@call","type":0,"sectionRef":"#","url":"/docs/directives/call/","content":"","keywords":"","version":"Next"},{"title":"steps​","type":1,"pageTitle":"@call","url":"/docs/directives/call/#steps","content":" @call directive can compose together other resolvers, allowing to create a chain of resolvers that can be executed in sequence. This is done by using the steps parameter, which is an array of objects that define the operations to be executed.  ","version":"Next","tagName":"h2"},{"title":"query​","type":1,"pageTitle":"@call","url":"/docs/directives/call/#query","content":" Specify the root query field to invoke, alongside the requisite arguments, using the @call directive for a concise and efficient query structure.  type Post { userId: Int! user: User @call( steps: [ {query: &quot;user&quot;, args: {id: &quot;{{value.userId}}&quot;}} ] ) }   ","version":"Next","tagName":"h3"},{"title":"mutation​","type":1,"pageTitle":"@call","url":"/docs/directives/call/#mutation","content":" Similarly, the @call directive can facilitate calling a mutation from another mutation field, employing the mutation parameter for field specification and the args parameter for argument delineation.  type Mutation { insertPost(input: PostInput, overwrite: Boolean): Post @http( body: &quot;{{args.input}}&quot; method: &quot;POST&quot; path: &quot;/posts&quot; query: {overwrite: &quot;{{args.overwrite}}&quot;} ) upsertPost(input: PostInput): Post @call( steps: [ { mutation: &quot;insertPost&quot; args: {input: &quot;{{args.input}}&quot;, overwrite: true} } ] ) }   ","version":"Next","tagName":"h3"},{"title":"args​","type":1,"pageTitle":"@call","url":"/docs/directives/call/#args","content":" The args parameter in the @call directive facilitates passing arguments to the targeted query or mutation, represented as a key-value mapping where each key corresponds to an argument name and its associated value.  type Post { userId: Int! user: User @call( steps: [ {query: &quot;user&quot;, args: {id: &quot;{{value.userId}}&quot;}} ] ) }   tip The @call directive is predominantly advantageous in complex, large-scale configurations. For those new to GraphQL or Tailcall, it may be beneficial to explore this directive after familiarizing yourself with the foundational aspects of GraphQL.  ","version":"Next","tagName":"h3"},{"title":"Composition​","type":1,"pageTitle":"@call","url":"/docs/directives/call/#composition","content":" @call directive provides the ability to express a sequence of steps that one might need to compose. These steps are executed such that the result of each step is passed as an argument to the next step. The query and mutation parameters are used to specify the target field, while the args parameter is used to pass arguments to the target field.  Let's explain this with an example:  schema @server(graphiql: true) { query: Query } type Query { a(input: JSON): JSON @expr(body: {value: &quot;{{args.input.a}}&quot;}) b(input: JSON): JSON @expr(body: {value: &quot;{{args.input.b}}&quot;}) c(input: JSON): JSON @expr(body: {value: &quot;{{args.input.c}}&quot;}) }   Here we have defined there operations viz. a, b &amp; c each of them pluck their respective keys from the given input value. Let's run this query with some test input:  { a(input: {a: 100}) b(input: {b: 200}) c(input: {c: 300}) }   Here is how the response would look like:  { &quot;data&quot;: { &quot;a&quot;: { &quot;value&quot;: 100 }, &quot;b&quot;: { &quot;value&quot;: 200 }, &quot;c&quot;: { &quot;value&quot;: 300 } } }   As you can see the @expr directive plucks the inner value and returns the result. How about we implement an abc operation that could leverage the existing operations and unwrap the following input value:  {&quot;a&quot;: {&quot;b&quot;: {&quot;c&quot;: {&quot;d&quot;: 1000}}}}   Given the above input if we wish to extract the last inner number 1000 then we could define a new operation as follows  schema @server(graphiql: true) { query: Query } type Query { a(input: JSON): JSON @expr(body: {value: &quot;{{args.input.a}}&quot;}) b(input: JSON): JSON @expr(body: {value: &quot;{{args.input.b}}&quot;}) c(input: JSON): JSON @expr(body: {value: &quot;{{args.input.c}}&quot;}) abc(input: JSON): JSON @call( steps: [ {query: &quot;a&quot;, args: {input: &quot;{{args.input}}&quot;}} {query: &quot;b&quot;, args: {input: &quot;{{args.value}}&quot;}} {query: &quot;c&quot;, args: {input: &quot;{{args.value}}&quot;}} ] ) }   We use the @call directive to compose the operations together. The args specify how we would like to pass the arguments to the operation and the result of that operation is passed to the next step. We can test the new abc operation with the following query:  query { abc(input: {a: {b: {c: 1000}}}) }   The server returns the response that we expected:  { &quot;data&quot;: { &quot;abc&quot;: { &quot;value&quot;: 100 } } }   This way you can compose combine multiple operations can compose them together using the @call directive.  note We use JSON scalar here because we don't care about the type safety of this option. In a real world example you might want to use proper input and output types. ","version":"Next","tagName":"h3"},{"title":"@modify","type":0,"sectionRef":"#","url":"/docs/directives/modify/","content":"","keywords":"","version":"Next"},{"title":"name​","type":1,"pageTitle":"@modify","url":"/docs/directives/modify/#name","content":" You can rename a field or a node in your GraphQL schema using the name argument in the @modify directive. This can be helpful when the field name in your underlying data source doesn't match the desired field name in your schema. For instance:  type User { id: Int! @modify(name: &quot;userId&quot;) }   @modify(name: &quot;userId&quot;) informs GraphQL to present the field known as id in the underlying data source as userId in your schema.  ","version":"Next","tagName":"h2"},{"title":"omit​","type":1,"pageTitle":"@modify","url":"/docs/directives/modify/#omit","content":" You can exclude a field or a node from your GraphQL schema using the omit argument in the @modify directive. This can be useful if you want to keep certain data hidden from the client. For instance:  type User { id: Int! @modify(omit: true) }   @modify(omit: true) instructs GraphQL to exclude the id field from the schema, making it inaccessible to the client.  tip @omit is a standalone directive and is an alias/shorthand for modify(omit: true) checkout documentation ","version":"Next","tagName":"h2"},{"title":"@omit","type":0,"sectionRef":"#","url":"/docs/directives/omit/","content":"","keywords":"","version":"Next"},{"title":"How it works​","type":1,"pageTitle":"@omit","url":"/docs/directives/omit/#how-it-works","content":" When applied to a field or node, the @omit directive instructs the Tailcall not to include that field or node in the schema. This means that clients cannot query or mutate data in those fields.  ","version":"Next","tagName":"h2"},{"title":"Example​","type":1,"pageTitle":"@omit","url":"/docs/directives/omit/#example","content":" Consider a scenario where you have a User type with an embedded Address type. If you want to exclude the Address type from the schema to simplify the API, you can use the @omit directive:  type Address { city: String street: String } type User { name: String address: Address @omit }   In this example, the address field will not be accessible or visible through the GraphQL API.  ","version":"Next","tagName":"h2"},{"title":"Comparison with modify​","type":1,"pageTitle":"@omit","url":"/docs/directives/omit/#comparison-with-modify","content":" The @omit directive and @modify(omit: true) essentially serve the same purpose in excluding fields from the schema, but they differ in syntax and flexibility. In fact, one can consider @omit as a shorthand or alias for the more verbose @modify(omit: true).  @omit offers a concise way to directly exclude a field or node without additional arguments. @modify(omit: true), as part of the broader @modify directive, provides more options, such as field renaming through the name argument. This makes it a more flexible choice when you need more than field exclusion. ","version":"Next","tagName":"h2"},{"title":"@protected","type":0,"sectionRef":"#","url":"/docs/directives/protected/","content":"","keywords":"","version":"Next"},{"title":"How It Works​","type":1,"pageTitle":"@protected","url":"/docs/directives/protected/#how-it-works","content":" When a field is annotated with @protected, an authentication check is performed upon receiving the request. Depending on the authentication result, either the requested data is provided in the response, or an authentication error is returned.If a type is annotated with @protected, all fields within that type inherit the protection, requiring user authentication for any field that's queried. ","version":"Next","tagName":"h2"},{"title":"@link","type":0,"sectionRef":"#","url":"/docs/directives/link/","content":"","keywords":"","version":"Next"},{"title":"How it Works​","type":1,"pageTitle":"@link","url":"/docs/directives/link/#how-it-works","content":" The @link directive requires specifying a source src, the resource's type type, and an optional identifier id.  src: The source of the link is defined here. It can be either a URL or a file path. When a file path is given, it's relative to the file's location that is importing the link. type: This specifies the link's type, which determines how the imported resource is integrated into the schema. For a list of supported types, see the Supported Types section. id: This is an optional field that assigns a unique identifier to the link. It's helpful for referring to the link within the schema.  ","version":"Next","tagName":"h2"},{"title":"Example​","type":1,"pageTitle":"@link","url":"/docs/directives/link/#example","content":" The following example illustrates how to utilize the @link directive to incorporate a Protocol Buffers (.proto) file for a gRPC service into your GraphQL schema.  schema @server(port: 8000, graphiql: true) @upstream( baseURL: &quot;http://news.local&quot; httpCache: true batch: {delay: 10} ) @link( id: &quot;news&quot; src: &quot;../src/grpc/news.proto&quot; type: Protobuf ) { query: Query } type Query { news: NewsData! @grpc(method: &quot;news.NewsService.GetAllNews&quot;) } type News { id: Int title: String body: String postImage: String } type NewsData { news: [News]! }   ","version":"Next","tagName":"h2"},{"title":"Supported Types​","type":1,"pageTitle":"@link","url":"/docs/directives/link/#supported-types","content":" The @link directive enriches your configuration by supporting the integration of external resources. Each link type is designed to serve a specific purpose, enhancing the functionality and flexibility of your schema. Below is a detailed overview of each supported link type:  ","version":"Next","tagName":"h2"},{"title":"Config​","type":1,"pageTitle":"@link","url":"/docs/directives/link/#config","content":" The Config link type is essential for importing other configuration files. This feature enables a modular approach to schema management by allowing configurations from the imported file to override overlapping settings in the main schema. This functionality is useful in large projects, where maintaining a single monolithic schema file becomes impractical. By using Config, developers can split their schema configurations into manageable pieces, thus promoting better organization and scalability.  Example use case:  Modularizing schema configurations for different environments (development, staging, production).Reusing common configurations across multiple schema files.  ","version":"Next","tagName":"h3"},{"title":"Protobuf​","type":1,"pageTitle":"@link","url":"/docs/directives/link/#protobuf","content":" The Protobuf link type integrates Protocol Buffers definitions by importing .proto files. This integration is crucial for Tailcall to communicate with gRPC services. By including .proto definitions, the Tailcall server can directly interact with gRPC services, allowing for efficient and type-safe communication.  For detailed integration steps and best practices, refer to the gRPC Integration Guide.  ","version":"Next","tagName":"h3"},{"title":"Script​","type":1,"pageTitle":"@link","url":"/docs/directives/link/#script","content":" The Script link type allows the config to link to an external JavaScript file. This file can contain custom logic that is executed in response to HTTP request-response events. This feature enables developers to implement custom behaviors, such as adding headers to responses or filtering requests based on specific criteria.  Example script for adding a custom header to all outgoing requests:  function onRequest({request}) { // Add a custom header for all outgoing requests request.headers[&quot;X-Custom-Header&quot;] = &quot;Processed&quot; // Return the updated request return {request} }   ","version":"Next","tagName":"h3"},{"title":"Cert​","type":1,"pageTitle":"@link","url":"/docs/directives/link/#cert","content":" The Cert link type is designed for importing SSL/TLS certificates, a crucial component for enabling HTTPS in your Tailcall server. This link type ensures that your Tailcall server can expose connections over HTTPS.  tip When using the Cert link type, specify the path to the certificate file. Ensure the certificate is up-to-date and issued by a trusted certificate authority (CA) to avoid security warnings or connection issues.  Example use case:  Securing communication between the Tailcall server and clients.Enhancing privacy and security by encrypting data in transit.  ","version":"Next","tagName":"h3"},{"title":"Key​","type":1,"pageTitle":"@link","url":"/docs/directives/link/#key","content":" The Key link type imports the private key associated with your SSL/TLS certificate, enabling HTTPS for your Tailcall server. The private key is a critical security element that decrypts information encrypted by the corresponding public key in the SSL/TLS certificate.  When configuring the Key link type, provide the path to your private key file. Ensure the private key matches the imported certificate specified by the Cert link above, and is protected by appropriate file permissions to maintain security.  ","version":"Next","tagName":"h3"},{"title":"Operation​","type":1,"pageTitle":"@link","url":"/docs/directives/link/#operation","content":" The Operation link type connects your schema to a set of predefined, GraphQL spec-compliant queries and mutations. This functionality allows for the validation and optimization of these operations by the Tailcall server.  Each type serves a specific purpose, enabling the flexible integration of external resources into your GraphQL schema.  ","version":"Next","tagName":"h3"},{"title":"Htpasswd​","type":1,"pageTitle":"@link","url":"/docs/directives/link/#htpasswd","content":" The Htpasswd link type allows the importation of an htpasswd file. This file is utilized to set up Basic authentication.  ","version":"Next","tagName":"h3"},{"title":"Jwks​","type":1,"pageTitle":"@link","url":"/docs/directives/link/#jwks","content":" The Jwks link type enables the importation of a JWKS file. This file facilitates the provision of detailed access control through JWT authentication. ","version":"Next","tagName":"h3"},{"title":"@http","type":0,"sectionRef":"#","url":"/docs/directives/http/","content":"","keywords":"","version":"Next"},{"title":"baseURL​","type":1,"pageTitle":"@http","url":"/docs/directives/http/#baseurl","content":" Specifies the API's base URL. If unspecified, it defaults to the URL in the @upstream directive.  type Query { users: [User] @http( path: &quot;/users&quot; baseURL: &quot;https://jsonplaceholder.typicode.com&quot; ) }   ","version":"Next","tagName":"h2"},{"title":"path​","type":1,"pageTitle":"@http","url":"/docs/directives/http/#path","content":" Refers to the API endpoint, for example, https://jsonplaceholder.typicode.com/users.  type Query { users: [User] @http(path: &quot;/users&quot;) }   If your API endpoint contains dynamic segments, you can substitute variables using Mustache templates. For example, to fetch a specific user, you can write the path as /users/{{args.id}}.  type Query { user(id: ID!): User @http(path: &quot;/users/{{args.id}}&quot;) }   ","version":"Next","tagName":"h2"},{"title":"method​","type":1,"pageTitle":"@http","url":"/docs/directives/http/#method","content":" Specifies the HTTP method for the API call. The default method is GET if not specified.  type Mutation { createUser(input: UserInput!): User @http(method: &quot;POST&quot;, path: &quot;/users&quot;) }   ","version":"Next","tagName":"h2"},{"title":"query​","type":1,"pageTitle":"@http","url":"/docs/directives/http/#query","content":" Represents the API call's query parameters, either as a static object or with dynamic parameters using Mustache templates. These parameters append to the URL.  type Query { userPosts(id: ID!): [Post] @http( path: &quot;/posts&quot; query: [{key: &quot;userId&quot;, value: &quot;{{args.id}}&quot;}] ) }   ","version":"Next","tagName":"h2"},{"title":"body​","type":1,"pageTitle":"@http","url":"/docs/directives/http/#body","content":" Defines the API call's body, necessary for methods like POST or PUT. Pass it as a static object or use Mustache templates for variable substitution from the GraphQL variables.  type Mutation { createUser(input: UserInput!): User @http( method: &quot;POST&quot; path: &quot;/users&quot; body: &quot;{{args.input}}&quot; ) }   In the example above, the createUser mutation sends a POST request to /users, with the input object converted to JSON and included in the request body.  ","version":"Next","tagName":"h2"},{"title":"headers​","type":1,"pageTitle":"@http","url":"/docs/directives/http/#headers","content":" Customizes the HTTP request headers made by the @http directive. Specify a key-value map of header names and values.  For instance:  type Mutation { createUser(input: UserInput!): User @http( path: &quot;/users&quot; headers: [{key: &quot;X-Server&quot;, value: &quot;Tailcall&quot;}] ) }   In this example, a request to /users will include a HTTP header X-Server with the value Tailcall.  You can make use of mustache templates to provide dynamic values for headers, derived from the arguments or context provided in the request. For example:  type Mutation { users(name: String): User @http( path: &quot;/users&quot; headers: [ {key: &quot;X-Server&quot;, value: &quot;Tailcall&quot;} {key: &quot;User-Name&quot;, value: &quot;{{args.name}}&quot;} ] ) }   In this scenario, the User-Name header's value will dynamically adjust according to the name argument passed in the request.  ","version":"Next","tagName":"h2"},{"title":"batchKey​","type":1,"pageTitle":"@http","url":"/docs/directives/http/#batchkey","content":" Groups data requests into a single call, enhancing efficiency. Refer to our n + 1 guide for more details.  type Post { id: Int! name: String! user: User @http( path: &quot;/users&quot; query: [{key: &quot;id&quot;, value: &quot;{{value.userId}}&quot;}] batchKey: [&quot;id&quot;] ) }   query: {key: &quot;id&quot;, value: &quot;{{value.userId}}&quot;}]: Instructs TailCall CLI to generate a URL aligning the user id with userId from the parent Post, compiling a single URL for a batch of posts, such as /users?id=1&amp;id=2&amp;id=3...id=10, consolidating requests into one. ","version":"Next","tagName":"h2"},{"title":"@rest","type":0,"sectionRef":"#","url":"/docs/directives/rest/","content":"","keywords":"","version":"Next"},{"title":"Usage​","type":1,"pageTitle":"@rest","url":"/docs/directives/rest/#usage","content":" method: Specifies the HTTP method (GET, POST, etc.).path: Sets the endpoint URL, with support for dynamic values from query arguments.query: Defines the query parameters as key-value pairs.  ","version":"Next","tagName":"h2"},{"title":"Example​","type":1,"pageTitle":"@rest","url":"/docs/directives/rest/#example","content":" Define GraphQL types and queries, using the @rest directive to map fields to REST API endpoints.  schema.graphql  schema @server(graphiql: true) @upstream(baseURL: &quot;https://jsonplaceholder.typicode.com&quot;) @link(type: Operation, src: &quot;user-operation.graphql&quot;) { query: Query } type Query { user(id: Int!): User @rest(method: &quot;GET&quot;, path: &quot;/users/{{args.id}}&quot;) } type User { id: Int! name: String! email: String! }   user-operation.graphql  query ($id: Int!) @rest(method: GET, path: &quot;/user/$id&quot;) { user(id: $id) { id name } }     This example demonstrates how to define a simple query to fetch user data from a REST endpoint using the @rest directive. By leveraging @rest, GraphQL can serve as a layer over RESTful services, combining REST's simplicity with GraphQL's flexibility. ","version":"Next","tagName":"h2"},{"title":"@telemetry","type":0,"sectionRef":"#","url":"/docs/directives/telemetry/","content":"","keywords":"","version":"Next"},{"title":"Traces​","type":1,"pageTitle":"@telemetry","url":"/docs/directives/telemetry/#traces","content":" Here are the traces that are captured by the @telemetry directive:  Trace Name\tDescriptionrequest\tCaptures the span for processing the HTTP request on the server side, providing foundational observability. graphQL\tOnly for GraphQL ingress. Span for processing GraphQL call REST &lt;http_method&gt; &lt;http_route&gt;\tOnly for REST ingress. Span for processing REST API call &lt;field_name&gt;\tDenotes spans for fields with defined resolvers, offering insights into field names and execution times for resolver logic. &lt;expr_name&gt;\tNested within the &lt;field_name&gt; spans, these granulated spans detail the execution of expressions in resolving a field, highlighting the hierarchical execution pattern of nested expressions. upstream_request\tRequest that were made from tailcall service to upstream  ","version":"Next","tagName":"h2"},{"title":"Metrics​","type":1,"pageTitle":"@telemetry","url":"/docs/directives/telemetry/#metrics","content":" The @telemetry directive also captures the following metrics:  Metric\tDescriptioncache.hit_rate\tReflects the cache hit rate for the cache powered by the @cache directive http.server.request.count\tCounts the number of incoming requests made to specific route. Optionally enriched with selected headers by requestHeaders http.client.request.count\tCounts the number of outgoing requests to specific upstream  ","version":"Next","tagName":"h2"},{"title":"export​","type":1,"pageTitle":"@telemetry","url":"/docs/directives/telemetry/#export","content":" The export field defines how the open-telemetry data should be exported and in which format. The following are the supported formats:  ","version":"Next","tagName":"h2"},{"title":"otlp​","type":1,"pageTitle":"@telemetry","url":"/docs/directives/telemetry/#otlp","content":" Utilizes the OTLP format to export telemetry data to backend systems, supported by most modern tracing and analytics platforms. Here is an example using [honeycomb.io]:  schema @telemetry( export: { otlp: { url: &quot;https://api.honeycomb.io:443&quot; headers: [ { key: &quot;x-honeycomb-team&quot; value: &quot;{{env.HONEYCOMB_API_KEY}}&quot; } {key: &quot;x-honeycomb-dataset&quot;, value: &quot;tailcall&quot;} ] } } ) { query: Query }   You can configure the OTLP exporter with the following options:  Field\tDescriptionurl\tDefines the URL for the OTLP Collector. headers\tSets additional headers for requests to the OTLP Collector.  ","version":"Next","tagName":"h3"},{"title":"prometheus​","type":1,"pageTitle":"@telemetry","url":"/docs/directives/telemetry/#prometheus","content":" Facilitates metrics export in a Prometheus compatible format, providing a dedicated endpoint for metrics.  schema @telemetry(export: {prometheus: {path: &quot;/metrics&quot;}}) { query: Query }   You can configure the Prometheus exporter with the following options:  Field\tDescriptionpath\tDesignates the endpoint path for Prometheus metrics, defaulting to /metrics. format\tControls the format viz. text or protobuf, for sending data to Prometheus.  ","version":"Next","tagName":"h3"},{"title":"stdout​","type":1,"pageTitle":"@telemetry","url":"/docs/directives/telemetry/#stdout","content":" Outputs all telemetry data to stdout, ideal for testing or local development environments.  schema @telemetry(export: {stdout: {pretty: true}}) { query: Query }   You can configure the stdout exporter with the following options:  Field\tDescriptionpretty\tEnables formatted output of telemetry data for enhanced readability.  ","version":"Next","tagName":"h3"},{"title":"requestHeaders​","type":1,"pageTitle":"@telemetry","url":"/docs/directives/telemetry/#requestheaders","content":" Specifies list of headers of ingress request the value of which will be sent to the telemetry as attributes.  schema @telemetry(requestHeaders: [&quot;X-User-Id&quot;]) { query: Query }   ","version":"Next","tagName":"h2"},{"title":"apollo​","type":1,"pageTitle":"@telemetry","url":"/docs/directives/telemetry/#apollo","content":" Facilitates seamless integration with Apollo Studio, enhancing the observability of GraphQL services. By leveraging this field, developers gain access to valuable insights into the performance and behavior of their GraphQL APIs.  schema @telemetry( export: { otlp: { api_key: &quot;{{env.APOLLO_API_KEY}}&quot; graph_ref: &quot;graph-id@current&quot; platform: &quot;website.com&quot; version: &quot;1.0.0&quot; } } ) { query: Query }   You can configure the apollo exporter with the following options:  Field\tDescriptionapi_key\tThe API Key generated from Apollo Studio. graph_ref\tThe Graph Ref, which is the graph_id and the variant concatenated using @(i.e. &lt;graph_id&gt;@&lt;variant&gt;) platform\tAn arbitrary value which can contain the name of your website or some other value to identify your deployment uniqely, in case you have multiple deployments. version\tVersion of Apollo which is being used.  By integrating the @telemetry directive into your GraphQL schema, you empower your development teams with critical insights into application performance, enabling proactive optimization and maintenance. ","version":"Next","tagName":"h2"},{"title":"Installation","type":0,"sectionRef":"#","url":"/docs/getting_started/","content":"","keywords":"","version":"Next"},{"title":"NPM​","type":1,"pageTitle":"Installation","url":"/docs/getting_started/#npm","content":" If you don't already have nodejs installed, you can find the instructions here. Install Tailcall by running the following command in your terminal: npm i -g @tailcallhq/tailcall To verify the correct installation of Tailcall, run: tailcall note Do not use the --force flag during npm installations, as it ignores installing platform-specific builds.  ","version":"Next","tagName":"h2"},{"title":"Yarn​","type":1,"pageTitle":"Installation","url":"/docs/getting_started/#yarn","content":" Install Tailcall by running the following command in your terminal: yarn global add @tailcallhq/tailcall To verify the correct installation of Tailcall, run: tailcall   ","version":"Next","tagName":"h2"},{"title":"Homebrew​","type":1,"pageTitle":"Installation","url":"/docs/getting_started/#homebrew","content":" If you don't already have Homebrew installed, you can find the instructions here. Add the Tailcall repository to Homebrew by running the following command in your terminal: brew tap tailcallhq/tailcall brew install tailcall To verify the correct installation of Tailcall, run: tailcall After completing the installation, perform upgrades with: brew update brew upgrade tailcall   ","version":"Next","tagName":"h2"},{"title":"Curl​","type":1,"pageTitle":"Installation","url":"/docs/getting_started/#curl","content":" Follow the steps below to manually install the cli on your system:  curl -sSL https://tailcall.run/install.sh | bash -s --   This command fetches and executes the Tailcall installation script. The ~/.tailcall directory contains the installed files.  Upon completion of the installation, extend your PATH environment variable to include the ~/.tailcall/bin directory:  export PATH=$PATH:~/.tailcall/bin   ","version":"Next","tagName":"h2"},{"title":"Docker​","type":1,"pageTitle":"Installation","url":"/docs/getting_started/#docker","content":" To install Tailcall with Docker, follow the steps below. Please note that currently, this installation method only works on Linux/amd64 systems. Before starting, make sure you have Docker installed on your system. If not, download it from here.  Pull the latest Tailcall Docker image using the following command: docker pull tailcall.docker.scarf.sh/tailcallhq/tailcall/tc-server: This command fetches the latest version of the Tailcall Docker image from the Docker registry. Run the Tailcall Docker container with the following command: docker run -d --name graphql-server -p 8000:8000 -v /path/to/your/configs:/etc/tailcall --entrypoint &quot;/bin/sh&quot; ghcr.io/tailcallhq/tailcall/tc-server: -c &quot;export PATH=$PATH:~/.tailcall/bin &amp;&amp; tailcall start /etc/tailcall/config.graphql&quot; This command launches the Tailcall server in a Docker container, exposing the GraphQL endpoint on port 8080. ","version":"Next","tagName":"h2"},{"title":"Execute","type":0,"sectionRef":"#","url":"/docs/getting_started/execute/","content":"Execute Open a web browser and go to http://localhost:8000. This should load the GraphiQL interface. In the query editor of GraphiQL, enter the following query query { users { id name posts { title } } } After running the query in GraphiQL, expect to see a JSON response structured like this: { &quot;data&quot;: { &quot;users&quot;: [ { &quot;id&quot;: 1, &quot;name&quot;: &quot;Leanne Graham&quot;, &quot;posts&quot;: [ { &quot;title&quot;: &quot;sunt aut facere repellat provident occaecati excepturi option reprehenderit&quot; } // Posts truncated for brevity ] }, { &quot;id&quot;: 2, &quot;name&quot;: &quot;Ervin Howell&quot;, &quot;posts&quot;: [ { &quot;title&quot;: &quot;et ea vero quia laudantium autem&quot; }, { &quot;title&quot;: &quot;in quibusdam tempore odit est dolorem&quot; } // Posts truncated for brevity ] } // Users truncated for brevity ] } } You can now add more fields, and compose more queries together!","keywords":"","version":"Next"},{"title":"Configuration","type":0,"sectionRef":"#","url":"/docs/getting_started/configuration/","content":"Configuration For our first example, we are going to compose a GraphQL schema from the REST APIs at https://jsonplaceholder.typicode.com, a free online REST API with some fake data. We will use the API at /users to get a list of users, and /users/:id/posts to get the posts for each user, and compose them into a single GraphQL schema. We can use the following formats to define our GraphQL schema: .graphql, .yml, .json. Create one of the following files and paste the contents into it. graphqlymljson schema # Specify server configuration: Start tailcall server at 0.0.0.0:8000 and enable GraphiQL playground @server(port: 8000, graphiql: true) # Specify a base url for all http requests @upstream(baseURL: &quot;http://jsonplaceholder.typicode.com&quot;) { query: Query } type Query { # Specify the http path for the users query users: [User] @http(path: &quot;/users&quot;) } # Create a user type with the fields returned by the users api type User { id: Int! name: String! username: String! email: String! # Extend the user type with the posts field # Use the current user's id to construct the path posts: [Post] @http(path: &quot;/users/{{value.id}}/posts&quot;) } # Create a post type with the fields returned by the posts api type Post { id: Int! title: String! body: String! } The above file is a standard .graphQL file, with some minor additions such as @upstream and @http directives. Basically we specify the GraphQL schema and how to resolve that GraphQL schema in the same file, without having to write any code!","keywords":"","version":"Next"},{"title":"Launch","type":0,"sectionRef":"#","url":"/docs/getting_started/launch/","content":"Launch Now, run the following command to start the server with the full path to the file that you created earlier. graphqlymljson tailcall start ./jsonplaceholder.graphql If the command succeeds, you should see logs like the following below. INFO File read: ./jsonplaceholder.graphql ... ok INFO N + 1 detected: 0 INFO 🚀 Tailcall launched at [0.0.0.0:8000] over HTTP/1.1 INFO 🌍 Playground: http://127.0.0.1:8000 The server starts with the schema provided and prints out a load of meta information. We will cover those in detail in a bit. For now, open the playground URL in a new tab in your browser and try it out for yourself!","keywords":"","version":"Next"},{"title":"@upstream","type":0,"sectionRef":"#","url":"/docs/directives/upstream/","content":"","keywords":"","version":"Next"},{"title":"poolIdleTimeout​","type":1,"pageTitle":"@upstream","url":"/docs/directives/upstream/#poolidletimeout","content":" The connection pool waits for this duration in seconds before closing idle connections.  schema @upstream( poolIdleTimeout: 60 baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h2"},{"title":"poolMaxIdlePerHost​","type":1,"pageTitle":"@upstream","url":"/docs/directives/upstream/#poolmaxidleperhost","content":" The max number of idle connections each host will maintain.  schema @upstream( poolMaxIdlePerHost: 60 baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h2"},{"title":"keepAliveInterval​","type":1,"pageTitle":"@upstream","url":"/docs/directives/upstream/#keepaliveinterval","content":" The time in seconds between each keep-alive message sent to maintain the connection.  schema @upstream( keepAliveInterval: 60 baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h2"},{"title":"keepAliveTimeout​","type":1,"pageTitle":"@upstream","url":"/docs/directives/upstream/#keepalivetimeout","content":" The time in seconds that the connection will wait for a keep-alive message before closing.  schema @upstream( keepAliveTimeout: 60 baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h2"},{"title":"keepAliveWhileIdle​","type":1,"pageTitle":"@upstream","url":"/docs/directives/upstream/#keepalivewhileidle","content":" A boolean value that determines whether to send keep-alive messages while the connection is idle.  schema @upstream( keepAliveWhileIdle: false baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h2"},{"title":"proxy​","type":1,"pageTitle":"@upstream","url":"/docs/directives/upstream/#proxy","content":" The proxy setting defines an intermediary server that routes upstream requests before they reach their intended endpoint. By specifying a proxy URL, you introduce a layer, enabling custom routing and security policies.  schema @upstream( proxy: {url: &quot;http://localhost:3000&quot;} baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query mutation: Mutation }   In the provided example, we've set the proxy's url to &quot;http://localhost:3000&quot;. This configuration ensures that all requests aimed at the designated baseURL first go through this proxy. To illustrate, if the baseURL is &quot;http://jsonplaceholder.typicode.com&quot;, any request targeting it initially goes to &quot;http://localhost:3000&quot; before the proxy redirects it to its final destination.  ","version":"Next","tagName":"h2"},{"title":"connectTimeout​","type":1,"pageTitle":"@upstream","url":"/docs/directives/upstream/#connecttimeout","content":" The time in seconds that the connection will wait for a response before timing out.  schema @upstream( connectTimeout: 60 baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h2"},{"title":"timeout​","type":1,"pageTitle":"@upstream","url":"/docs/directives/upstream/#timeout","content":" The max time in seconds that the connection will wait for a response.  schema @upstream( timeout: 60 baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h2"},{"title":"tcpKeepAlive​","type":1,"pageTitle":"@upstream","url":"/docs/directives/upstream/#tcpkeepalive","content":" The time in seconds between each TCP keep-alive message sent to maintain the connection.  schema @upstream( tcpKeepAlive: 60 baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h2"},{"title":"userAgent​","type":1,"pageTitle":"@upstream","url":"/docs/directives/upstream/#useragent","content":" The User-Agent header value for HTTP requests.  schema @upstream( userAgent: &quot;Tailcall/1.0&quot; baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h2"},{"title":"allowedHeaders​","type":1,"pageTitle":"@upstream","url":"/docs/directives/upstream/#allowedheaders","content":" The allowedHeaders configuration defines a set of whitelisted HTTP headers that can be forwarded to upstream services during requests. Without specifying allowedHeaders, the system will not forward any incoming headers to upstream services, offering an extra security layer but potentially limiting necessary data flow. Tailcall compares the provided whitelisted headers in a case-insensitive format.  schema @upstream( allowedHeaders: [&quot;Authorization&quot;, &quot;X-Api-Key&quot;] ) { query: Query mutation: Mutation }   In the example above, the configuration for allowedHeaders permits Authorization and X-Api-Key headers. Thus, requests with these headers will forward them to upstream services; the system ignores all others. This configuration ensures communication of the expected headers to dependent services, emphasizing security and consistency.  ","version":"Next","tagName":"h2"},{"title":"baseURL​","type":1,"pageTitle":"@upstream","url":"/docs/directives/upstream/#baseurl","content":" This refers to the default base URL for your APIs. If it's not explicitly mentioned in the @upstream directive, then each @http directive must specify its own baseURL. If neither @upstream nor @http provides a baseURL, it results in a compilation error.  schema @upstream( baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query mutation: Mutation }   In this representation, http://jsonplaceholder.typicode.com serves as the baseURL. Thus, all API calls made by @http prepend this URL to their respective paths.  tip Ensure that your base URL remains free from specific path segments. GOOD: @upstream(baseURL: http://jsonplaceholder.typicode.com)BAD: @upstream(baseURL: http://jsonplaceholder.typicode.com/api)  ","version":"Next","tagName":"h2"},{"title":"httpCache​","type":1,"pageTitle":"@upstream","url":"/docs/directives/upstream/#httpcache","content":" When activated, directs Tailcall to use HTTP caching mechanisms, following the HTTP Caching RFC to enhance performance by minimizing unnecessary data fetches. If left unspecified, this feature defaults to false.  schema @upstream(httpCache: false) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h2"},{"title":"Tips​","type":1,"pageTitle":"@upstream","url":"/docs/directives/upstream/#tips","content":" Use batching when other optimization techniques fail to resolve performance issues.Apply batching and thoroughly assess its impact.Understand that batching may make debugging more challenging.  ","version":"Next","tagName":"h3"},{"title":"batch​","type":1,"pageTitle":"@upstream","url":"/docs/directives/upstream/#batch","content":" An object that specifies the batch settings, including maxSize (the max size of the batch), delay (the delay in milliseconds between each batch), and headers (an array of HTTP headers that the batch will include).  schema @upstream( batch: { maxSize: 1000 delay: 10 headers: [&quot;X-Server&quot;, &quot;Authorization&quot;] } ) { query: Query mutation: Mutation }  ","version":"Next","tagName":"h2"},{"title":"@server","type":0,"sectionRef":"#","url":"/docs/directives/server/","content":"","keywords":"","version":"Next"},{"title":"workers​","type":1,"pageTitle":"@server","url":"/docs/directives/server/#workers","content":" Setting workers to 32 means that the Tailcall server will use 32 worker threads.  schema @server(workers: 32) { query: Query mutation: Mutation }   This example sets the workers to 32, meaning the Tailcall server will use 32 worker threads.  ","version":"Next","tagName":"h2"},{"title":"port​","type":1,"pageTitle":"@server","url":"/docs/directives/server/#port","content":" Setting the port to 8090 means that Tailcall will be accessible at http://localhost:8000.  schema @server(port: 8090) { query: Query mutation: Mutation }   This example sets the port to 8090, making Tailcall accessible at http://localhost:8090.  tip Always choose non-standard ports, avoiding typical ones like 80 or 8080. Make sure your chosen port is free.  ","version":"Next","tagName":"h2"},{"title":"headers​","type":1,"pageTitle":"@server","url":"/docs/directives/server/#headers","content":" Allows intelligent configuration of the final response headers that's produced by Tailcall.  ","version":"Next","tagName":"h2"},{"title":"cacheControl​","type":1,"pageTitle":"@server","url":"/docs/directives/server/#cachecontrol","content":" Activating the cacheControl configuration directs Tailcall to send Cache-Control headers in its responses. The max-age value in the header matches the lowest of the values in the responses that Tailcall receives from its upstream. By default, this is false, which means Tailcall does not set any header.  schema @server(headers: {cacheControl: true}) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h3"},{"title":"custom​","type":1,"pageTitle":"@server","url":"/docs/directives/server/#custom","content":" The custom is an array of key-value pairs. These headers get added to the response of every request made to the server. This can be useful for adding headers like Access-Control-Allow-Origin to allow cross-origin requests, or some headers like X-Allowed-Roles for use by downstream services.  schema @server( headers: { custom: [ {key: &quot;X-Allowed-Roles&quot;, value: &quot;admin,user&quot;} ] } ) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h3"},{"title":"experimental​","type":1,"pageTitle":"@server","url":"/docs/directives/server/#experimental","content":" When the experimental configuration is enabled, Tailcall can include headers starting with X- in its responses, which are sourced from its upstream. By default, this feature is disabled ([]), meaning Tailcall does not forward any such headers unless explicitly configured to do so.  schema @server( headers: {experimental: [&quot;X-Experimental-Header&quot;]} ) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h3"},{"title":"setCookies​","type":1,"pageTitle":"@server","url":"/docs/directives/server/#setcookies","content":" Enabling the setCookies option instructs Tailcall to include set-cookie headers in its responses, which are obtained from the headers of upstream responses.  schema @server(headers: {setCookies: true}) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h3"},{"title":"cors​","type":1,"pageTitle":"@server","url":"/docs/directives/server/#cors","content":" The cors configuration allows you to enable CORS on Tailcall. This is useful when you want to access Tailcall in the browser. It has the following fields:  allowCredentials: Indicates whether the server allows credentials (e.g., cookies, authorization headers) to be sent in cross-origin requests.allowHeaders: A list of allowed headers in cross-origin requests. This can be used to specify custom headers that are allowed to be included in cross-origin requests.allowMethods: A list of allowed HTTP methods in cross-origin requests. These methods specify the actions that are permitted in cross-origin requests.allowOrigins: A list of origins that are allowed to access the server's resources in cross-origin requests. An origin can be a domain, a subdomain, or even 'null' for local file schemes.allowPrivateNetwork: Indicates whether requests from private network addresses are allowed in cross-origin requests. Private network addresses typically include IP addresses reserved for internal networks.exposeHeaders: A list of headers that the server exposes to the browser in cross-origin responses. Exposing certain headers allows client-side code to access them in the response.maxAge: The maximum time (in seconds) that the client should cache preflight OPTIONS requests to avoid sending excessive requests to the server.vary: A list of header names that indicate the values of which might cause the server's response to vary, potentially affecting caching.  schema @server( port: 8000 graphiql: true hostname: &quot;0.0.0.0&quot; headers: { cors: { allowCredentials: false allowHeaders: [&quot;Authorization&quot;] allowMethods: [POST, GET, OPTIONS] allowOrigins: [&quot;abc.xyz&quot;] allowPrivateNetwork: true exposeHeaders: [&quot;Content-Type&quot;] maxAge: 360 vary: &quot;Origin&quot; } } ) { query: Query }   ","version":"Next","tagName":"h3"},{"title":"graphiql​","type":1,"pageTitle":"@server","url":"/docs/directives/server/#graphiql","content":" Enabling the graphiql configuration activates the GraphiQL IDE at the root (/) path within Tailcall. GraphiQL is a built-in, interactive in-browser GraphQL IDE, designed to streamline query development and testing. By default, this feature is off.  schema @server(port: 8000, graphiql: true) { query: Query mutation: Mutation }   tip While the GraphiQL interface is a powerful tool for development, consider disabling it in production environments if you're not exposing GraphQL APIs directly to users. This ensures an added layer of security and reduces unnecessary exposure.  ","version":"Next","tagName":"h2"},{"title":"vars​","type":1,"pageTitle":"@server","url":"/docs/directives/server/#vars","content":" This configuration allows defining local variables for use during the server's operations. These variables are handy for storing constant configurations, secrets, or other shared information that operations might need.  schema @server( vars: {key: &quot;apiKey&quot;, value: &quot;YOUR_API_KEY_HERE&quot;} ) { query: Query mutation: Mutation } type Query { externalData: Data @http( path: &quot;/external-api/data&quot; headers: [ { key: &quot;Authorization&quot; value: &quot;Bearer {{vars.apiKey}}&quot; } ] ) }   In the provided example, setting a variable named apiKey with a placeholder value of &quot;YOUR_API_KEY_HERE&quot; implies that whenever Tailcall fetches data from the externalData endpoint, it includes the apiKey in the Authorization header of the HTTP request.  tip Local variables, like apiKey, are instrumental in securing access to external services or providing a unified place for configurations. Ensure that sensitive information stored this way is well protected and not exposed unintentionally, if your Tailcall configuration is publicly accessible.  ","version":"Next","tagName":"h2"},{"title":"introspection​","type":1,"pageTitle":"@server","url":"/docs/directives/server/#introspection","content":" This setting controls the server's allowance of introspection queries. Introspection, a core feature of GraphQL, allows clients to directly fetch schema information. This capability proves crucial for tools and client applications in comprehending the available types, fields, and operations. By default, the server enables this setting (true).  schema @server(introspection: false) { query: Query mutation: Mutation }   tip Although introspection is beneficial during development and debugging stages, consider disabling it in production environments. Turning off introspection in live deployments can enhance security by preventing potential attackers from discerning the schema and any associated business logic or data structures.  ","version":"Next","tagName":"h2"},{"title":"queryValidation​","type":1,"pageTitle":"@server","url":"/docs/directives/server/#queryvalidation","content":" The queryValidation configuration determines if the server checks incoming GraphQL queries against the defined schema. Each query check ensures it matches the schema, preventing errors from incorrect or malformed queries. In some situations, you might want to disable it, notably to enhance server performance at the cost of these checks. This defaults to false if not specified.  schema @server(queryValidation: true) { query: Query mutation: Mutation }   The example above sets queryValidation to true, enabling the validation phase for incoming queries.  tip Enable this in the development environment to ensure the queries sent are correct and validated. In the production environment, consider disabling it for improved performance.  ","version":"Next","tagName":"h2"},{"title":"responseValidation​","type":1,"pageTitle":"@server","url":"/docs/directives/server/#responsevalidation","content":" Tailcall can automatically infer the schema of the HTTP endpoints for you. This information can check responses received from the upstream services. Enabling this setting allows you to do that. If not specified, the default setting for responseValidation is false.  schema @server(responseValidation: true) { query: Query mutation: Mutation }   tip Disabling this setting will offer major performance improvements, but at the potential expense of data integrity.  ","version":"Next","tagName":"h2"},{"title":"globalResponseTimeout​","type":1,"pageTitle":"@server","url":"/docs/directives/server/#globalresponsetimeout","content":" The globalResponseTimeout configuration sets the max duration a query can run before the server terminates it. Essentially, it acts as a safeguard against long-running queries that could strain resources or pose security concerns.  If not explicitly defined, there might be a system-specific or default value that applies.  schema @server(globalResponseTimeout: 5000) { query: Query mutation: Mutation }   In this given example, setting the globalResponseTimeout to 5000 milliseconds, or 5 seconds, means any query execution taking longer than this duration will be automatically terminated by  tip Setting an appropriate response timeout in production environments is crucial. This optimizes resource use and serves as a security measure against potential denial-of-service attacks, where adversaries might run complex queries to exhaust server resources.  ","version":"Next","tagName":"h2"},{"title":"version​","type":1,"pageTitle":"@server","url":"/docs/directives/server/#version","content":" The server uses the HTTP version. If not specified, the default value is HTTP1. The available options are HTTP1 and HTTP2.  schema @server(version: HTTP2) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h2"},{"title":"cert​","type":1,"pageTitle":"@server","url":"/docs/directives/server/#cert","content":" The path to certificate(s) for running the server over HTTP2 (HTTPS). If not specified, the default value is null.  schema @server(cert: &quot;./cert.pem&quot;) { query: Query mutation: Mutation }   tip The certificate can be of any extension, but it's highly recommended to use standards (pem, crt, key).  ","version":"Next","tagName":"h2"},{"title":"key​","type":1,"pageTitle":"@server","url":"/docs/directives/server/#key","content":" The path to the key for running the server over HTTP2 (HTTPS). If not specified, the default value is null.  schema @server(key: &quot;./key.pem&quot;) { query: Query mutation: Mutation }   tip The key can be of any extension, but it's highly recommended to use standards (pem, crt, key).  ","version":"Next","tagName":"h2"},{"title":"showcase​","type":1,"pageTitle":"@server","url":"/docs/directives/server/#showcase","content":" The @server directive's showcase option allows for hands-on experimentation with server configurations in a controlled environment. This feature simplifies the process of exploring and testing different settings.  schema @server(showcase: true) { query: Query }   ","version":"Next","tagName":"h3"},{"title":"batchRequests​","type":1,"pageTitle":"@server","url":"/docs/directives/server/#batchrequests","content":" Batching in GraphQL combines requests into one, reducing server round trips.  schema @server( port: 8000 batchRequests: true )   ","version":"Next","tagName":"h2"},{"title":"Trade-offs​","type":1,"pageTitle":"@server","url":"/docs/directives/server/#trade-offs","content":" Batching can improve performance but may introduce latency if one request in the batch takes longer. It also makes network traffic debugging harder. ","version":"Next","tagName":"h3"},{"title":"CLI","type":0,"sectionRef":"#","url":"/docs/guides/cli/","content":"","keywords":"","version":"Next"},{"title":"check​","type":1,"pageTitle":"CLI","url":"/docs/guides/cli/#check","content":" The check command validates a composition spec. Notably, this command can detect potential N+1 issues. To use the check command, follow this format:  tailcall check [options] &lt;file&gt;...   The check command offers options that control settings such as the display of the generated schema, n + 1 issues etc.  ","version":"Next","tagName":"h2"},{"title":"--n-plus-one-queries​","type":1,"pageTitle":"CLI","url":"/docs/guides/cli/#--n-plus-one-queries","content":" This flag triggers the detection of N+1 issues.  Type: BooleanDefault: false  tailcall check --n-plus-one-queries &lt;file&gt;...   ","version":"Next","tagName":"h3"},{"title":"--schema​","type":1,"pageTitle":"CLI","url":"/docs/guides/cli/#--schema","content":" This option enables the display of the schema of the composition spec.  Type: BooleanDefault: false  tailcall check --schema &lt;file1&gt; &lt;file2&gt; ... &lt;fileN&gt;   The check command allows for files. Specify each file path, separated by a space, after the options.  Example:  tailcall check --schema ./path/to/file1.graphql ./path/to/file2.graphql   ","version":"Next","tagName":"h3"},{"title":"--format​","type":1,"pageTitle":"CLI","url":"/docs/guides/cli/#--format","content":" This is an optional command which allows changing the format of the input file. It accepts gql or graphql,yml or yaml, json .  tailcall check ./path/to/file1.graphql ./path/to/file2.graphql --format json   ","version":"Next","tagName":"h3"},{"title":"start​","type":1,"pageTitle":"CLI","url":"/docs/guides/cli/#start","content":" The start command launches the TailCall Server, acting as a GraphQL proxy with specific configurations. The server can publish GraphQL configurations.  To start the server, use the following command:  tailcall start &lt;file1&gt; &lt;file2&gt; ... &lt;fileN&gt; &lt;http_path1&gt; &lt;http_path2&gt; .. &lt;http_pathN&gt;   The start command allows for files and supports loading configurations over HTTP. You can mix file system paths with HTTP paths. Specify each path, separated by a space, after the options.  Example:  tailcall start ./path/to/file1.graphql ./path/to/file2.graphql http://example.com/file2.graphql   ","version":"Next","tagName":"h2"},{"title":"init​","type":1,"pageTitle":"CLI","url":"/docs/guides/cli/#init","content":" The init command bootstraps a new TailCall project. It creates the necessary GraphQL schema files in the provided file path.  tailcall init &lt;file_path&gt;   This command prompts for file creation and configuration, creating the following files:  File Name\tDescription.tailcallrc.schema.json\tProvides autocomplete in your editor when the configuration is written in json or yml format. .graphqlrc.yml\tA GraphQL Configuration that references your Tailcall config (if it's in .graphql format) and the following .tailcallrc.graphql. .tailcallrc.graphql\tContains Tailcall specific auto-completions for .graphql format.  ","version":"Next","tagName":"h2"},{"title":"gen​","type":1,"pageTitle":"CLI","url":"/docs/guides/cli/#gen","content":" The gen command in the TailCall CLI is designed for generating TailCall configurations from one or more source files.  ","version":"Next","tagName":"h2"},{"title":"--input​","type":1,"pageTitle":"CLI","url":"/docs/guides/cli/#--input","content":" Supported input formats include PROTO.  ","version":"Next","tagName":"h3"},{"title":"--output​","type":1,"pageTitle":"CLI","url":"/docs/guides/cli/#--output","content":" Output is same as --format, it supports graphql, json and yaml as output type  Example:  tailcall gen &lt;file1&gt; &lt;file2&gt; ... &lt;fileN&gt; --input proto --output gql  ","version":"Next","tagName":"h3"},{"title":"Client Tuning","type":0,"sectionRef":"#","url":"/docs/guides/client-tuning/","content":"","keywords":"","version":"Next"},{"title":"HTTP (Hypertext Transfer Protocol)​","type":1,"pageTitle":"Client Tuning","url":"/docs/guides/client-tuning/#http-hypertext-transfer-protocol","content":" HTTP, the most widely used protocol for communication between clients and servers, carries your request to the server and then brings back the data to your client. TCP forms the foundation of HTTP.  ","version":"Next","tagName":"h3"},{"title":"HTTP Versions: 1.x, 2, and 3​","type":1,"pageTitle":"Client Tuning","url":"/docs/guides/client-tuning/#http-versions-1x-2-and-3","content":" Each version has enhanced HTTP's flexibility and performance.  HTTP/1.x: Creates a separate TCP connection for each HTTP request (or reuses one sequentially).HTTP/2: Introduces multiplexing to allow concurrent sending of requests and responses over a single TCP connection, enhancing performance.HTTP/3: Employs QUIC instead of TCP, further reducing connection setup time and improving packet loss and network change handling.  note The server determines the HTTP version. Thus, if the server supports HTTP/1, the client cannot make an HTTP/2 request, even if compatible. If the client supports HTTP/1, the server should, according to the specification, downgrade to serve the request over HTTP/1.  ","version":"Next","tagName":"h3"},{"title":"TCP (Transmission Control Protocol)​","type":1,"pageTitle":"Client Tuning","url":"/docs/guides/client-tuning/#tcp-transmission-control-protocol","content":" TCP ensures the data sent and received over the internet reaches its destination and in order.  TCP, like dialing a number before talking on the phone, establishes a connection between the client and server before exchanging data using HTTP. This guide will show how to tune Tailcall's HTTP client to enhance this connection's performance. Learn more about TCP in detail here.  ","version":"Next","tagName":"h3"},{"title":"QUIC (Quick UDP Internet Connections)​","type":1,"pageTitle":"Client Tuning","url":"/docs/guides/client-tuning/#quic-quick-udp-internet-connections","content":" Developed by Google, QUIC aims to make web communications faster and more efficient than TCP. It reduces connection establishment time, handles packet loss better, and supports multiplexed streams over a single connection, preventing a slow request from holding up others. HTTP/3 uses QUIC. Learn more about QUIC in detail here.  ","version":"Next","tagName":"h3"},{"title":"Why Managing Connections is Important?​","type":1,"pageTitle":"Client Tuning","url":"/docs/guides/client-tuning/#why-managing-connections-is-important","content":" Performance Overhead: Establishing TCP connections with HTTP/1.x consumes time due to the complete TCP handshake for each new connection. This process adds latency and increases system resources. Limited Ports on Client Side: A unique combination of an IP address and a port number is necessary for each TCP connection from a client. With each new connection, the IP remains the same because the client is the same, but a new port gets used. The number of available ports on a machine is 65535. These ports get shared among all processes, and not all are available for use. Excessive creation of new connections can lead to port exhaustion on the client side, preventing new connections and causing system failures across running processes. tip Use lsof and netstat commands to check the ports to process mapping.  Connection pooling mitigates these issues by reusing existing connections for requests, reducing connection establishment frequency (and thus handshake overhead) and conserving client-side ports. This approach enhances application performance by minimizing the resources and time spent on managing connections.  ","version":"Next","tagName":"h3"},{"title":"Tuning HTTP Client​","type":1,"pageTitle":"Client Tuning","url":"/docs/guides/client-tuning/#tuning-http-client","content":" Tailcall uses connection pooling by default and sets up with default tuning suitable for most use cases. You might need to further tune the HTTP client to improve your application's performance. Tailcall DSL provides a directive named @upstream for this purpose.  note Connection pooling optimizes HTTP/1. Since HTTP/2 and HTTP/3 support multiplexing, pooling enabled does not noticeably affect performance.  When using HTTP/1.x, tune the connection pool with the following parameters:  ","version":"Next","tagName":"h2"},{"title":"poolMaxIdlePerHost​","type":1,"pageTitle":"Client Tuning","url":"/docs/guides/client-tuning/#poolmaxidleperhost","content":" poolMaxIdlePerHost specifies the allowed number of idle connections per host, defaulting to 60. Example:  schema @upstream( poolMaxIdlePerHost: 60 ) { query: Query }   Too idle connections can unnecessarily consume memory and ports, while too few might cause delays as new connections need frequent establishment. poolMaxIdlePerHost ensures judicious use of network and memory resources, avoiding wastage on seldom-used connections.  For applications connecting to hosts, set this value lower to keep connections available for other hosts. Conversely, if you have hosts and all requests must resolve through them, maintain a higher value for this setting.  ","version":"Next","tagName":"h3"},{"title":"tcpKeepAlive​","type":1,"pageTitle":"Client Tuning","url":"/docs/guides/client-tuning/#tcpkeepalive","content":" tcpKeepAlive keeps TCP connections alive for a duration, during inactivity, by periodically sending packets to the server to check if the connection remains open. In connection pooling, tcpKeepAlive maintains reusable connections in a ready-to-use state. This setting is useful for long-lived connections, preventing -lived connections, preventing the client from using a connection the server has closed due to inactivity. Without tcpKeepAlive, connections in the pool might get dropped by the server or intermediate network devices (like firewalls or load balancers). When your client tries to use such a dropped connection, it would fail, causing delays and errors. Keeping connections alive and monitored means you can efficiently reuse them, reducing the overhead of establishing new connections frequently.  Tailcall provides a parameter named tcpKeepAlive for the upstream which defaults to 5 seconds. Example: schema  @upstream ( tcpKeepAlive: 300 ) { query: Query }   ","version":"Next","tagName":"h3"},{"title":"connectTimeout​","type":1,"pageTitle":"Client Tuning","url":"/docs/guides/client-tuning/#connecttimeout","content":" connectTimeout specifically applies to the phase where your client attempts to establish a connection with the server. When making a connection request, the client tries to resolve the DNS, complete the SSL handshake, and establish a TCP connection. In environments where pods are frequently created and destroyed, maintaining a low connectTimeout is crucial to avoid unnecessary delays. In systems using connection pooling, the system aborts the attempt if it cannot establish a connection within the connectTimeout period. This approach prevents indefinite waiting for a connection to establish, which could cause delays and timeouts.  Tailcall offers a connectTimeout parameter to set the connection timeout in seconds for the HTTP client, defaulting to 60 seconds. Example:  schema @upstream( connectTimeout: 10 ) { query: Query }   In summary, maximizing HTTP client performance requires understanding the underlying protocols and configuring client settings through testing. This ensures efficient, robust, and high-performing client-server communication, crucial for the smooth operation of modern web applications. ","version":"Next","tagName":"h3"},{"title":"Context Overview","type":0,"sectionRef":"#","url":"/docs/guides/context/","content":"","keywords":"","version":"Next"},{"title":"Schema Definition​","type":1,"pageTitle":"Context Overview","url":"/docs/guides/context/#schema-definition","content":" type Context = { args: Map&lt;string, JSON&gt; value: JSON env: Map&lt;string, string&gt; vars: Map&lt;string, string&gt; headers: Map&lt;string, string&gt; }   Context operates by storing values as key-value pairs, which can be accessed through mustache template syntax.  ","version":"Next","tagName":"h2"},{"title":"args​","type":1,"pageTitle":"Context Overview","url":"/docs/guides/context/#args","content":" This property facilitates access to query arguments. Consider the example:  type Query { user(id: ID!): User @http(path: &quot;/users/{{args.id}}&quot;) }   Here, args.id is utilized to retrieve the id argument provided to the user query.  ","version":"Next","tagName":"h3"},{"title":"value​","type":1,"pageTitle":"Context Overview","url":"/docs/guides/context/#value","content":" This enables access to the fields of the specified type.  type Post { id: ID! title: String! body: String! comments: [Comment] @http(path: &quot;/posts/{{value.id}}/comments&quot;) }   In this case, value.id accesses the id field of the Post type.  ","version":"Next","tagName":"h3"},{"title":"env​","type":1,"pageTitle":"Context Overview","url":"/docs/guides/context/#env","content":" Environment variables, set at server startup, allow directives to dynamically adapt behavior based on external configurations without altering the server configuration itself.  Example:  type Query { users: [User]! @http(baseUrl: &quot;{{env.API_ENDPOINT}}&quot;, path: &quot;/users&quot;) }   env.API_ENDPOINT references an environment variable named API_ENDPOINT, which specifies the base URL for HTTP requests.  ","version":"Next","tagName":"h3"},{"title":"vars​","type":1,"pageTitle":"Context Overview","url":"/docs/guides/context/#vars","content":" vars offers a mechanism for defining reusable variables within the configuration. Unlike env, these are embedded and can be universally applied across configurations.  schema @server( vars: {key: &quot;apiKey&quot;, value: &quot;{{env.AUTH_TOKEN}}&quot;} ) { query: Query } type Query { user(id: ID!): [User] @http( url: &quot;/users&quot; headers: [ { key: &quot;Authorization&quot; value: &quot;Bearer {{vars.apiKey}}&quot; } ] ) }   Here, the variable apiKey is set using an environment variable and subsequently utilized in the Authorization header for HTTP requests.  ","version":"Next","tagName":"h3"},{"title":"headers​","type":1,"pageTitle":"Context Overview","url":"/docs/guides/context/#headers","content":" Headers originate from the request made to the Tailcall server.  type Query { commentsForUser: [Comment] @http(path: &quot;/users/{{headers.x-user-id}}/comments&quot;) }   In this example, headers.x-user-id extracts the value of the x-user-id header present in the request, dynamically constructing the request path. ","version":"Next","tagName":"h3"},{"title":"Naming Conventions","type":0,"sectionRef":"#","url":"/docs/guides/conventions/","content":"","keywords":"","version":"Next"},{"title":"General Naming Principles​","type":1,"pageTitle":"Naming Conventions","url":"/docs/guides/conventions/#general-naming-principles","content":" Consistency is Key: Ensure that naming conventions are uniform across your entire schema to maintain clarity and consistency.Descriptive Over Generic: Opt for descriptive, specific names rather than broad, generic ones to avoid ambiguity.Avoid Abbreviations: Avoid the use of acronyms, initialism, and abbreviations to keep your schema intuitive and understandable.  ","version":"Next","tagName":"h2"},{"title":"Detailed Naming Cases​","type":1,"pageTitle":"Naming Conventions","url":"/docs/guides/conventions/#detailed-naming-cases","content":" ","version":"Next","tagName":"h2"},{"title":"Fields, Arguments, and Directives​","type":1,"pageTitle":"Naming Conventions","url":"/docs/guides/conventions/#fields-arguments-and-directives","content":" Adopt camelCase: Utilize camelCase for field names, argument names, and directive names to achieve a clear, consistent structure.  type Query { postTitle(userId: Int): String } directive @includeIf on FIELD   ","version":"Next","tagName":"h3"},{"title":"Types​","type":1,"pageTitle":"Naming Conventions","url":"/docs/guides/conventions/#types","content":" Prefer PascalCase: Use PascalCase for defining types, enabling easy identification and differentiation.  type Post { ... } enum StatusEnum { ... } interface UserInterface { ... } union SearchResult = ... scalar Date   Enum Values in SCREAMING_SNAKE_CASE: Distinguish enum values by using SCREAMING_SNAKE_CASE.  enum StatusEnum { PUBLISHED DRAFT }   ","version":"Next","tagName":"h3"},{"title":"Field Naming Best Practices​","type":1,"pageTitle":"Naming Conventions","url":"/docs/guides/conventions/#field-naming-best-practices","content":" ","version":"Next","tagName":"h2"},{"title":"Queries​","type":1,"pageTitle":"Naming Conventions","url":"/docs/guides/conventions/#queries","content":" Avoid get or list Prefixes: Refrain from using prefixes like get or list in your query names to ensure predictability and consistency.  type Query { # 👎 Avoid getPosts: [Post] # 👍 Prefer posts: [Post] }   Maintain consistency between root and nested fields:  # 👎 Avoid query PostQuery { getPosts { id getUser { name } } } # 👍 Prefer query PostQuery { posts { id user { name } } }   ","version":"Next","tagName":"h3"},{"title":"Mutations​","type":1,"pageTitle":"Naming Conventions","url":"/docs/guides/conventions/#mutations","content":" Verb Prefixes for Mutations: Begin mutation field names with a verb to indicate the action being performed, improving schema readability.  type Mutation { # 👎 Avoid postAdd(input: AddPostInput): AddPostPayload! # 👍 Prefer addPost(input: AddPostInput): AddPostPayload! }   ","version":"Next","tagName":"h3"},{"title":"Type Naming Conventions​","type":1,"pageTitle":"Naming Conventions","url":"/docs/guides/conventions/#type-naming-conventions","content":" ","version":"Next","tagName":"h2"},{"title":"Input Types​","type":1,"pageTitle":"Naming Conventions","url":"/docs/guides/conventions/#input-types","content":" Input Suffix: Denote input types by appending Input to their names, specifying their use case.  input AddPostInput { title: String! body: String! userId: Int! }   ","version":"Next","tagName":"h3"},{"title":"Output Types​","type":1,"pageTitle":"Naming Conventions","url":"/docs/guides/conventions/#output-types","content":" Response or Payload Suffix: Use a consistent suffix like Response or Payload for the output types resulting from mutations.  type Mutation { addPost(input: AddPostInput!): AddPostResponse! } type AddPostResponse { success: Boolean! post: Post }   ","version":"Next","tagName":"h3"},{"title":"Advanced Naming Strategies​","type":1,"pageTitle":"Naming Conventions","url":"/docs/guides/conventions/#advanced-naming-strategies","content":" ","version":"Next","tagName":"h2"},{"title":"Resolving Namespace Conflicts​","type":1,"pageTitle":"Naming Conventions","url":"/docs/guides/conventions/#resolving-namespace-conflicts","content":" For addressing naming conflicts across different domains within your schema:  Use PascalCase Prefix: Distinguish similar types from distinct domains for clear separation without resorting to underscores. This method ensures a cleaner, more professional look while maintaining the integrity and readability of your schema.  type BlogPost { ... } type ForumPost { ... }   ","version":"Next","tagName":"h3"},{"title":"Conclusion​","type":1,"pageTitle":"Naming Conventions","url":"/docs/guides/conventions/#conclusion","content":" Implementing a consistent, descriptive, and intuitive naming convention is crucial for developing an understandable and maintainable GraphQL schema. By following the best practices outlined you can improve the clarity and effectiveness of your schema. ","version":"Next","tagName":"h2"},{"title":"Configuration Format","type":0,"sectionRef":"#","url":"/docs/guides/configuration/","content":"","keywords":"","version":"Next"},{"title":"Converting Formats​","type":1,"pageTitle":"Configuration Format","url":"/docs/guides/configuration/#converting-formats","content":" To convert files between different formats, use the following command:  tailcall check &lt;input_files&gt; --format format_type   Let's try to convert a Tailcall graphql file to json and then back to graphql  To convert graphql to json  tailcall check examples/jsonplaceholder.graphql --format json &gt; &quot;examples/jsonplaceholder.json&quot;   Now to convert back to graphql  tailcall check examples/jsonplaceholder.json --format graphql &gt; &quot;examples/jsonplaceholder2.graphql&quot;   To learn more about writing configuration to leverage the full power of Tailcall, explore the Directives documentation.  ","version":"Next","tagName":"h2"},{"title":"Format Conversions​","type":1,"pageTitle":"Configuration Format","url":"/docs/guides/configuration/#format-conversions","content":" graphqlymljson schema @server(port: 8000, graphiql: true, hostname: &quot;0.0.0.0&quot;) @upstream(baseURL: &quot;http://jsonplaceholder.typicode.com&quot;, httpCache: true, batch: {delay: 100}) { query: Query } type Query { posts: [Post] @http(path: &quot;/posts&quot;) users: [User] @http(path: &quot;/users&quot;) user(id: Int!): User @http(path: &quot;/users/{{args.id}}&quot;) } type User { id: Int! name: String! username: String! email: String! phone: String website: String } type Post { id: Int! userId: Int! title: String! body: String! user: User @call(query: &quot;user&quot;, args: {id: &quot;{{value.userId}}&quot;}) }   ","version":"Next","tagName":"h2"},{"title":"Editor Support​","type":1,"pageTitle":"Configuration Format","url":"/docs/guides/configuration/#editor-support","content":" To leverage autocomplete and validation for Tailcall configurations, the init command can be used to automatically create .tailcallrc.graphql for GraphQL configurations and .tailcallrc.schema.json for JSON and YAML configurations. These files enhance editor support by providing schema definitions, facilitating faster and error-free configuration.  ","version":"Next","tagName":"h2"},{"title":"GraphQL​","type":1,"pageTitle":"Configuration Format","url":"/docs/guides/configuration/#graphql","content":" When you run tailcall init, it creates a .tailcallrc.graphql file containing your GraphQL schema definitions and a .graphqlrc.yml file configured to use this schema. The .graphqlrc.yml file is set up as follows:  schema: - &quot;./.tailcallrc.graphql&quot; - &quot;./app.graphql&quot;   This file contains the path to the .tailcallrc.graphql file and the path to the main Tailcall configuration file which is app.graphql. This setup allows GraphQL IDE plugins and Language Server Protocols (LSP) to automatically pick up the schema for autocomplete and validation, enhancing your development experience with real-time feedback and suggestions.  ","version":"Next","tagName":"h3"},{"title":"JSON & YAML​","type":1,"pageTitle":"Configuration Format","url":"/docs/guides/configuration/#json--yaml","content":" For JSON or YAML configurations, tailcall init also creates a .tailcallrc.schema.json file. To enable validation and autocomplete in your JSON files, reference the .tailcallrc.schema.json in the $schema attribute at the beginning of your JSON file:  { &quot;$schema&quot;: &quot;./.tailcallrc.schema.json&quot; }   This reference enables your IDE to validate and autocomplete using the JSON schema, offering a streamlined configuration process with instant error and typo detection. ","version":"Next","tagName":"h3"},{"title":"Environment Variables","type":0,"sectionRef":"#","url":"/docs/guides/environment-variables/","content":"","keywords":"","version":"Next"},{"title":"Need for Environment Variables​","type":1,"pageTitle":"Environment Variables","url":"/docs/guides/environment-variables/#need-for-environment-variables","content":" Applications rely on external tools, authentication methods, and configurations. For proper functioning, our code needs to access these values.  Consider a scenario of JWT authentication. When signing tokens for our users, we need:  Expiry time: The duration after which the token expires.Secret key: The key for encrypting the token.Issuer: The token issuer, often the organization's name.  There are two ways to manage this:  Hardcode the values in our code: This approach, while simple, poses a massive security risk by exposing sensitive information and requires code changes and application redeployment for updates. Store the values in environment variables: Storing sensitive values in the OS of the server running your application allows runtime access without code modifications, keeping sensitive information secure and simplifying value changes.  ","version":"Next","tagName":"h2"},{"title":"Environment Variables​","type":1,"pageTitle":"Environment Variables","url":"/docs/guides/environment-variables/#environment-variables","content":" With Tailcall, you can seamlessly integrate environment variables into your GraphQL schema. Tailcall supports this through a env Context variable. All directives share this Context, allowing you to resolve values in your schema.  Example schema:  type Query { users: [User]! @http( baseUrl: &quot;https://jsonplaceholder.typicode.com&quot; path: &quot;/users&quot; ) }   Here, we fetch a list of users from the JSONPlaceholder API. The users field will contain the fetched value at runtime. This works fine, but what if we want to change the API endpoint? We would need to update the code and redeploy the application, which is cumbersome.  We can address this issue using environment variables. Replace the API endpoint with an environment variable, allowing us to change the variable's value without altering our codebase.  type Query { users: [User]! @http(baseUrl: &quot;{{env.API_ENDPOINT}}&quot;, path: &quot;/users&quot;) }   Here, you must set API_ENDPOINT as an environment variable on the device running your server. Upon startup, the server retrieves this value and makes it accessible through the env Context variable.  This approach allows us to change the API endpoint without modifying our codebase. For instance, we might use different API endpoints for development (stage-api.example.com) and production (api.example.com) environments.  Remember, environment variables are not limited to the baseUrl or @http directive. You can use them throughout your schema, as a Mustache template handles their evaluation.  Here's another example, using an environment variable in the headers of @grpc:  type Query { users: [User] @grpc( service: &quot;UserService&quot; method: &quot;ListUsers&quot; protoPath: &quot;./proto/user_service.proto&quot; baseURL: &quot;https://grpc-server.example.com&quot; headers: [ {key: &quot;X-API-KEY&quot;, value: &quot;{{env.API_KEY}}&quot;} ] ) }   ","version":"Next","tagName":"h2"},{"title":"Security Aspects and Best Practices​","type":1,"pageTitle":"Environment Variables","url":"/docs/guides/environment-variables/#security-aspects-and-best-practices","content":" Environment variables help reduce security risks, but it's crucial to understand that they do not remove these risks entirely because the values are in plain text. Even if configuration values are not always highly sensitive, there is still a potential for compromising secrets. To ensure your secrets remain secure, consider the following tips:  Use a .env file: It's a common practice to create a .env file in your project's root directory for storing all environment variables. Avoid committing this file to your version control system; instead, add it to .gitignore to prevent public exposure of your secrets. For clarity and collaboration, maintain a .env.example file that enumerates all the necessary environment variables for your application, thereby guiding other developers on what variables they need to set. Within Tailcall (or in other environments), you can make use of this .env file by exporting its key-value pairs to your operating system. For example, if your .env file looks like this: API_ENDPOINT=https://jsonplaceholder.typicode.com Export it to your OS with: export $(cat .env | xargs) On Windows: Get-Content .env | Foreach-Object { [System.Environment]::SetEnvironmentVariable($_.Split(&quot;=&quot;)[0], $_.Split(&quot;=&quot;)[1], &quot;User&quot;) } After this, you can access API_ENDPOINT in your codebase. Use Kubernetes Secrets: When deploying your application with Kubernetes, use its Secrets feature to manage environment variables. This approach ensures your secrets remain private and are not embedded in your codebase, while also making it easier to update values as necessary. Store Secrets Through Cloud Provider GUIs: For deployments using a cloud provider, use their GUI for environment variable management. These interfaces are intuitive and practical for containerized applications that automatically scale.  Following these practices ensures effective and secure management of your environment variables. ","version":"Next","tagName":"h2"},{"title":"Http Cache","type":0,"sectionRef":"#","url":"/docs/guides/http-cache/","content":"","keywords":"","version":"Next"},{"title":"Understanding HTTP Caching​","type":1,"pageTitle":"Http Cache","url":"/docs/guides/http-cache/#understanding-http-caching","content":" HTTP Caching involves saving copies of HTTP responses to serve identical future requests directly from the cache, bypassing the need for new API calls. This reduces latency, conserves bandwidth, and alleviates the load on upstream services by utilizing a cache keyed by request URLs and headers.  By default, HTTP caching is turned off in Tailcall. Enabling it requires setting the httpCache parameter to true in the @upstream configuration. Tailcall employs a in-memory Least_Recently_Used (LRU) cache mechanism to manage stored responses, adhering to upstream-provided caching directives like Cache-Control to optimize the caching process and minimize redundant upstream API requests.  ","version":"Next","tagName":"h3"},{"title":"Enabling HTTP Caching​","type":1,"pageTitle":"Http Cache","url":"/docs/guides/http-cache/#enabling-http-caching","content":" To activate HTTP caching, adjust the upstream configuration in Tailcall by setting httpCache to true, as shown in the following example:  schema @server(port: 4000) @upstream( baseURL: &quot;https://api.example.com&quot; httpCache: true ) { query: Query }   This configuration instructs Tailcall to cache responses from the designated upstream API.  ","version":"Next","tagName":"h3"},{"title":"Cache-Control headers in responses​","type":1,"pageTitle":"Http Cache","url":"/docs/guides/http-cache/#cache-control-headers-in-responses","content":" Enabling the cacheControl setting in Tailcall ensures that Cache-Control headers are included in the responses returned to clients. When activated, Tailcall dynamically sets the max-age directive in the Cache-Control header to the minimum max-age value encountered in any of the responses from upstream services. This approach guarantees that the caching duration for the composite response is conservative, aligning with the shortest cache validity period provided by the upstream services. By default, this feature is disabled (false), meaning Tailcall will not modify or add Cache-Control headers unless explicitly instructed to do so. This setting is distinct from the general HTTP cache setting, which controls whether responses are cached internally by Tailcall; cacheControl specifically controls the caching instructions sent to clients.  Here is how you can enable the cacheControl setting within your Tailcall schema to apply these caching instructions:  schema @server(headers: {cacheControl: true}) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h3"},{"title":"Best Practices for Enhancing REST API Performance with Tailcall​","type":1,"pageTitle":"Http Cache","url":"/docs/guides/http-cache/#best-practices-for-enhancing-rest-api-performance-with-tailcall","content":" The combination of httpCache and cacheControl provides a comprehensive caching solution. While httpCache focuses on internal caching to reduce the impact of high latency and frequent requests, cacheControl manages client-side caching policies, ensuring an optimal balance between performance, data freshness, and efficient resource use.  These caching primitives are beneficial for REST APIs that are latency-sensitive, have a high rate of request repetition, or come with explicit caching headers indicating cacheable responses. Together, they tackle the common challenges of optimizing REST API performance by minimizing unnecessary network traffic and server load while ensuring response accuracy.  To further enhance the performance of any API with Tailcall, integrating the @cache directive offers protocol agnostic control over caching at the field level within a GraphQL schema. ","version":"Next","tagName":"h3"},{"title":"Sequencing & Parallelism","type":0,"sectionRef":"#","url":"/docs/guides/execution-strategy/","content":"","keywords":"","version":"Next"},{"title":"Examples​","type":1,"pageTitle":"Sequencing & Parallelism","url":"/docs/guides/execution-strategy/#examples","content":" ","version":"Next","tagName":"h2"},{"title":"Example 1: Fetching a Specific User and Their Posts​","type":1,"pageTitle":"Sequencing & Parallelism","url":"/docs/guides/execution-strategy/#example-1-fetching-a-specific-user-and-their-posts","content":" Imagine you're building a blog and want to display a specific user's profile page containing their information and all their posts.  Schema:  type Query { # Retrieve a specific user by ID user(id: Int!): User @http(path: &quot;/users/{{value.id}}&quot;) } type User { id: Int! name: String! username: String! email: String! # Access user's posts using their ID in the path posts: [Post] @http(path: &quot;/users/{{value.id}}/posts&quot;) } type Post { id: Int! title: String! body: String! }   GraphQL Query:  query getUserAndPosts($userId: Int!) { # Fetch the user by ID user(id: $userId) { id name username email # Sequentially retrieve all posts for the fetched user posts { id title body } } }   Tailcall understands that retrieving the user's posts depends on knowing the user's ID, which is obtained in the first step. Therefore, it automatically fetches the user first and then uses their ID to retrieve all their posts in a sequential manner.  ","version":"Next","tagName":"h3"},{"title":"Example 2: Searching Multiple Posts and Users by ID​","type":1,"pageTitle":"Sequencing & Parallelism","url":"/docs/guides/execution-strategy/#example-2-searching-multiple-posts-and-users-by-id","content":" Suppose you're building a social media platform and want to display profiles of specific users and their recent posts.  Schema:  type Query { # Retrieve users from the &quot;/users&quot; endpoint users: [User] @http(path: &quot;/users&quot;) } type User { id: Int! name: String! username: String! email: String! # Access user's posts using their ID in the path posts: [Post] @http(path: &quot;/users/{{value.id}}/posts&quot;) } type Post { id: Int! title: String! body: String! }   GraphQL Query:  query getUsersWithLatestPosts { # Retrieve all users users { id name username email # Access user's posts through the nested field posts { id title body } } }   This query retrieves details of multiple users and their most recent posts based on the provided user IDs. Tailcall recognizes that fetching user details and their individual posts are independent tasks. As a result, it can execute these requests concurrently for each user.  ","version":"Next","tagName":"h3"},{"title":"Example 3: Fetching Posts with Users​","type":1,"pageTitle":"Sequencing & Parallelism","url":"/docs/guides/execution-strategy/#example-3-fetching-posts-with-users","content":" Imagine you're building a social media platform and want to display a list of posts with each post's author. Traditionally, you might write a query that retrieves all posts and then, for each post, make a separate request to fetch its corresponding user. This approach leads to the N+1 problem, where N represents the number of posts, and 1 represents the additional request per post to retrieve its user.  Schema:  type Query { posts: [Post] @http(path: &quot;/posts&quot;) } type Post { id: Int! userId: Int! title: String! body: String! user: User @http(path: &quot;/users/{{value.userId}}&quot;) } type User { id: Int! name: String! }   GraphQL Query:  query getPostsWithUsers { posts { id userId title body user { id name } } }   Tailcall analyzes the schema and recognizes that fetching user details for each post is independent. It can potentially execute these requests to /users/{{value.userId}} concurrently, fetching user data for multiple posts simultaneously.  In summary, Tailcall automates the management of sequence and parallelism in API calls. It analyzes the defined schema to optimize execution, freeing developers from manual intervention. ","version":"Next","tagName":"h3"},{"title":"HTTP Filters","type":0,"sectionRef":"#","url":"/docs/guides/http-filters/","content":"","keywords":"","version":"Next"},{"title":"Getting Started​","type":1,"pageTitle":"HTTP Filters","url":"/docs/guides/http-filters/#getting-started","content":" To leverage this functionality, a JavaScript function named onRequest must be created in a worker.js file. This function serves as middleware, allowing for the interception and modification of the request. Here is a simple example of a worker.js file that logs the request and returns the original request without any modifications.  function onRequest({request}) { console.log(`${request.method} ${request.uri.path}`) return {request} }   Once you have a worker file ready, you link that file to the tailcall configuration using the @link directive.  schema @link(type: Script, src: &quot;./worker.js&quot;) { query: Query }   Once the worker is linked, you can start the server using the usual start command. Making requests to tailcall will now be intercepted by the worker and logged to the console.  ","version":"Next","tagName":"h2"},{"title":"Modify Request​","type":1,"pageTitle":"HTTP Filters","url":"/docs/guides/http-filters/#modify-request","content":" You can modify the request by returning a request object from the onRequest function. Below is an example where we are modifying the request to add a custom header.  function onRequest({request}) { request.headers[&quot;x-custom-header&quot;] = &quot;Hello, Tailcall!&quot; return {request} }   ","version":"Next","tagName":"h2"},{"title":"Create Response​","type":1,"pageTitle":"HTTP Filters","url":"/docs/guides/http-filters/#create-response","content":" You can respond with custom responses by returning a response object from the onRequest function. Below is an example where we are responding with a custom response for all requests that start with https://api.example.com.  function onRequest({request}) { if (request.uri.path.startsWith(&quot;https://api.example.com&quot;)) { return { response: { status: 200, headers: { &quot;content-type&quot;: &quot;application/json&quot; }, body: JSON.stringify({message: &quot;Hello, Tailcall!&quot;}) } } } else { return {request} }   ","version":"Next","tagName":"h2"},{"title":"Response Redirect​","type":1,"pageTitle":"HTTP Filters","url":"/docs/guides/http-filters/#response-redirect","content":" Sometimes you might want to redirect the request to a different URL. You can do this by returning a response object with a status of 301 or 302 and a Location header. The following example redirects all requests to https://example.com to https://tailcall.com.  function onRequest({request}) { if (request.uri.path.startsWith(&quot;https://example.com&quot;)) { return { response: { status: 301, headers: { Location: &quot;https://tailcall.com&quot;, }, }, } } else { return {request} } }   important The new request that's created as a result of the redirect will not be intercepted by the worker.  ","version":"Next","tagName":"h2"},{"title":"Schema​","type":1,"pageTitle":"HTTP Filters","url":"/docs/guides/http-filters/#schema","content":" The onRequest function takes a single argument that contains the request object. The return value of the onRequest function can be a request object, or a response object. It can not be null or undefined.  ","version":"Next","tagName":"h2"},{"title":"Request​","type":1,"pageTitle":"HTTP Filters","url":"/docs/guides/http-filters/#request","content":" The request object has the following shape:  type Request = { method: string uri: { path: string query?: {[key: string]: string} scheme: &quot;Http&quot; | &quot;Https&quot; host?: string port?: number } headers: {[key: string]: string} body?: string }   By default the headers field will be empty in most cases, unless headers are whitelisted via the allowedHeaders setting in @upstream.  The http filter doesn't have access to the request's body, hence you can't directly modify the body of an outgoing request. This is more of a design choice than a limitation we have made to ensure that developers don't misuse this API to write all kind of business logic in Tailcall.  tip As an escape hatch you can pass the request body as a query param instead of an actual request body and read in the JS.  The modified request that's returned from the above onRequest function can optionally provide the body. This body is used by Tailcall as the request body while making the upstream request.  ","version":"Next","tagName":"h3"},{"title":"Response​","type":1,"pageTitle":"HTTP Filters","url":"/docs/guides/http-filters/#response","content":" The response object has the following shape:  type Response = { status: number headers: {[key: string]: string} body?: string }  ","version":"Next","tagName":"h3"},{"title":"Logging","type":0,"sectionRef":"#","url":"/docs/guides/logging/","content":"","keywords":"","version":"Next"},{"title":"error​","type":1,"pageTitle":"Logging","url":"/docs/guides/logging/#error","content":" This is the highest severity level. It indicates a critical issue that may lead to the failure of the program or a part of it.  TAILCALL_LOG_LEVEL=error tailcall &lt;COMMAND&gt; # or TC_LOG_LEVEL=error tailcall &lt;COMMAND&gt;   ","version":"Next","tagName":"h3"},{"title":"warn​","type":1,"pageTitle":"Logging","url":"/docs/guides/logging/#warn","content":" This log level signifies potential issues or warnings that do not necessarily result in immediate failure but may require attention.  TAILCALL_LOG_LEVEL=warn tailcall &lt;COMMAND&gt; # or TC_LOG_LEVEL=warn tailcall &lt;COMMAND&gt;   ","version":"Next","tagName":"h3"},{"title":"info​","type":1,"pageTitle":"Logging","url":"/docs/guides/logging/#info","content":" This level offers general information about the program's execution, providing insights into its state and activities.  TAILCALL_LOG_LEVEL=info tailcall &lt;COMMAND&gt; # or TC_LOG_LEVEL=info tailcall &lt;COMMAND&gt;   ","version":"Next","tagName":"h3"},{"title":"debug​","type":1,"pageTitle":"Logging","url":"/docs/guides/logging/#debug","content":" The debug log level is useful for developers during the debugging process, providing detailed information about the program's internal workings.  TAILCALL_LOG_LEVEL=debug tailcall &lt;COMMAND&gt; # or TC_LOG_LEVEL=debug tailcall &lt;COMMAND&gt;   ","version":"Next","tagName":"h3"},{"title":"trace​","type":1,"pageTitle":"Logging","url":"/docs/guides/logging/#trace","content":" The trace log level is the most detailed logging level, used for fine-grained debugging. This level provides exhaustive details about the program's execution flow.  TAILCALL_LOG_LEVEL=trace tailcall &lt;COMMAND&gt; # or TC_LOG_LEVEL=trace tailcall &lt;COMMAND&gt;   ","version":"Next","tagName":"h3"},{"title":"off​","type":1,"pageTitle":"Logging","url":"/docs/guides/logging/#off","content":" This level serves as a special indicator for generating no logs, allowing the option to disable logging entirely.  TAILCALL_LOG_LEVEL=off tailcall &lt;COMMAND&gt; # or TC_LOG_LEVEL=off tailcall &lt;COMMAND&gt;   info The default log level is info.  Log levels are hierarchical, meaning if you set the log level to a specific level, it includes all the levels above it. For example, setting the log level to info will include logs at the info, warn, and error levels, but exclude debug and trace logs.    info You can specify log levels in either uppercase or lowercase; both yield the same result. For example, TAILCALL_LOG_LEVEL=DEBUG and TAILCALL_LOG_LEVEL=debug are same. ","version":"Next","tagName":"h3"},{"title":"HTTP/2","type":0,"sectionRef":"#","url":"/docs/guides/http2/","content":"","keywords":"","version":"Next"},{"title":"SSL​","type":1,"pageTitle":"HTTP/2","url":"/docs/guides/http2/#ssl","content":" For Tailcall to serve GraphQL over HTTP/2 we need to first enable SSL for which we need to generate a certificate and a key. To generate the required certificates (cert.pem and key.pem) OpenSSL is a widely used option. Here are the steps to get started with SSL:  Install OpenSSL: Download and install OpenSSL from its official website if it's not already installed on your system. Generate Private Key openssl genrsa -out key.pem 2048 This creates a 2048-bit RSA private key, storing it in a file named key.pem. Generate Certificate Signing Request (CSR) openssl req -new -key key.pem -out csr.pem You will be prompted to provide information for the certificate, such as the Common Name (CN), organization details, and locality. This information is embedded into the CSR, saved in a file named csr.pem. This file can be used to request a certificate from a Certificate Authority (CA) or generate a self-signed certificate. Generate Self-Signed Certificate openssl x509 -req -days 365 -in csr.pem -signkey key.pem -out cert.pem This generates a self-signed certificate valid for 365 days using the CSR from step 3 and the private key from step 2. The validity period can be adjusted by changing the number of days (-days). A &quot;Signature ok&quot; prompt confirms the successful creation. Cleanup Intermediate Files rm csr.pem After using the CSR to generate the self-signed certificate (cert.pem), the CSR file (csr.pem) becomes redundant. This step removes intermediate files created during the certificate generation process.  tip Use self-signed certificates for HTTP/2 configurations in development environments. While they enable convenient HTTPS testing locally, in production, always opt for certificates issued by trusted Certificate Authorities.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"HTTP/2","url":"/docs/guides/http2/#configuration","content":" Once the certificate and key are generated we can link them with our main configuration using the @link directive, to enable HTTPS.  schema @link(type: &quot;Cert&quot;, src: &quot;./cert.pem&quot;) @link(type: &quot;Key&quot;, src: &quot;./key.pem&quot;) { query: Query mutation: Mutation } type Query { posts: [Post] @http(path: &quot;/posts&quot;) } type User { id: Int! name: String! }   Once HTTPS is enabled we set the version to HTTP2 for the server:  schema @link(type: &quot;Cert&quot;, src: &quot;./cert.pem&quot;) @link(type: &quot;Key&quot;, src: &quot;./key.pem&quot;) @server(version: HTTP2) { query: Query mutation: Mutation } type Query { posts: [Post] @http(path: &quot;/posts&quot;) } type User { id: Int! name: String! }   That's pretty much all that's required. Now you can go ahead and launch your server as usual.  INFO File read: ./jsonplaceholder.graphql ... ok INFO N + 1 detected: 0 INFO 🚀 Tailcall launched at [0.0.0.0:8000] over HTTP/2.0 INFO 🌍 Playground: http://127.0.0.1:8000  ","version":"Next","tagName":"h2"},{"title":"N+1 Problem","type":0,"sectionRef":"#","url":"/docs/guides/n+1/","content":"","keywords":"","version":"Next"},{"title":"Example Scenario​","type":1,"pageTitle":"N+1 Problem","url":"/docs/guides/n+1/#example-scenario","content":" Imagine we're enhancing a feature that involves fetching data from the [JSON Placeholder API], requiring posts and their authors' details.  ","version":"Next","tagName":"h2"},{"title":"Fetching Posts​","type":1,"pageTitle":"N+1 Problem","url":"/docs/guides/n+1/#fetching-posts","content":" Initially, we request all posts:  curl https::/jsonplaceholder.typicode.com/posts [ { &quot;userId&quot;: 1, &quot;id&quot;: 1, &quot;title&quot;: &quot;Creating Solutions for Challenges&quot;, &quot;body&quot;: &quot;We anticipate and understand challenges, creating solutions while considering exceptions and criticisms.&quot; }, { &quot;userId&quot;: 1, &quot;id&quot;: 2, &quot;title&quot;: &quot;Understanding Identity&quot;, &quot;body&quot;: &quot;Life's essence, measured through time, presents pains and joys. We find solace in the mundane, seeking meaning beyond the visible.&quot; } ]   This command retrieves posts from the API, with each post containing a userId field denoting its author.  ","version":"Next","tagName":"h3"},{"title":"Fetching Authors​","type":1,"pageTitle":"N+1 Problem","url":"/docs/guides/n+1/#fetching-authors","content":" Subsequently, we fetch details for each post's author, exemplified by:  curl https://jsonplaceholder.typicode.com/users/1 { &quot;id&quot;: 1, &quot;name&quot;: &quot;Leanne Graham&quot;, &quot;username&quot;: &quot;Bret&quot;, &quot;email&quot;: &quot;Sincere@april.biz&quot;, &quot;address&quot;: { &quot;street&quot;: &quot;Kulas Light&quot;, &quot;suite&quot;: &quot;Apt. 556&quot;, &quot;city&quot;: &quot;Gwenborough&quot;, &quot;zipcode&quot;: &quot;92998-3874&quot;, &quot;geo&quot;: { &quot;lat&quot;: &quot;-37.3159&quot;, &quot;lng&quot;: &quot;81.1496&quot; } }, &quot;phone&quot;: &quot;1-770-736-8031 x56442&quot;, &quot;website&quot;: &quot;hildegard.org&quot;, &quot;company&quot;: { &quot;name&quot;: &quot;Romaguera-Crona&quot;, &quot;catchPhrase&quot;: &quot;Multi-layered client-server neural-net&quot;, &quot;bs&quot;: &quot;harness real-time e-markets&quot; } }   For 100 posts, this leads to 100 additional requests to obtain author details, totaling 101 requests.  The N+1 issue with the JSON Placeholder API arises when fetching 100 posts prompts 100 separate requests for author details, cumulating to 101 requests.  info This issue escalates in real-life scenarios with thousands of posts and users, multiplying the server requests, which strains resources, hikes server costs, slows response times, and can even cause server downtime. Addressing the N+1 issue during application design and development is crucial for efficient API usage. We will examine solutions to mitigate this problem.  ","version":"Next","tagName":"h3"},{"title":"Initial Configuration with TailCall CLI​","type":1,"pageTitle":"N+1 Problem","url":"/docs/guides/n+1/#initial-configuration-with-tailcall-cli","content":" Before diving into solutions, it's insightful to see the N+1 problem in action using the TailCall CLI with the initial .graphql configuration:  ❯ tailcall start ./examples/jsonplaceholder.graphql INFO File read: ./examples/jsonplaceholder.graphql ... ok INFO N + 1 detected: 1 INFO 🚀 Tailcall launched at [0.0.0.0:8000] over HTTP/1.1 INFO 🌍 Playground: http://127.0.0.1:8000 INFO GET http://jsonplaceholder.typicode.com/posts HTTP/1.1 INFO GET http://jsonplaceholder.typicode.com/users/8 HTTP/1.1 ... INFO GET http://jsonplaceholder.typicode.com/users/10 HTTP/1.1   These logs demonstrate the sequence of requests made to fetch posts and then individual users, highlighting the N+1 problem in real-time.  ","version":"Next","tagName":"h2"},{"title":"Optimizing with Batching​","type":1,"pageTitle":"N+1 Problem","url":"/docs/guides/n+1/#optimizing-with-batching","content":" After incorporating the batchKey optimization, let's observe the transformed behavior through the TailCall CLI:  ❯ tailcall start ./examples/jsonplaceholder.graphql INFO File read: ./examples/jsonplaceholder.graphql ... ok INFO N + 1 detected: 0 INFO 🚀 Tailcall launched at [0.0.0.0:8000] over HTTP/1.1 INFO 🌍 Playground: http://127.0.0.1:8000 INFO GET http://jsonplaceholder.typicode.com/posts HTTP/1.1 INFO GET http://jsonplaceholder.typicode.com/users?id=1&amp;id=10&amp;id=2&amp;id=3&amp;id=4&amp;id=5&amp;id=6&amp;id=7&amp;id=8&amp;id=9 HTTP/1.1   ","version":"Next","tagName":"h2"},{"title":"The Impact of Optimization​","type":1,"pageTitle":"N+1 Problem","url":"/docs/guides/n+1/#the-impact-of-optimization","content":" The logs now reveal a substantial reduction in requests. Initially, fetching 100 posts would lead to an additional 100 requests to gather author details, totaling 101 requests. However, after applying batching with the batchKey, we observe two requests:  A single request to fetch all posts.A single consolidated request for all users involved in the posts, using query parameters to fetch multiple users in one go.  This optimization starkly contrasts the initial approach by reducing the number of server requests, thus minimizing server load, enhancing response times, and potentially lowering server costs. It demonstrates the effectiveness of addressing the N+1 problem through thoughtful schema design and the use of advanced tooling like the TailCall CLI, which facilitates a more efficient data fetching strategy that's crucial for scalable and performant web applications. ","version":"Next","tagName":"h3"},{"title":"Playground","type":0,"sectionRef":"#","url":"/docs/guides/playground/","content":"","keywords":"","version":"Next"},{"title":"Performance and Security​","type":1,"pageTitle":"Playground","url":"/docs/guides/playground/#performance-and-security","content":" Performance Impact: The showcase feature prioritizes flexibility and ease of testing over speed, leading to slower response times due to the overhead of dynamically applied configurations.Security Risk: There's a potential security risk as it may allow unauthorized access to files and environment variables.  important Due to these concerns, this mode is not recommended for production environments. ","version":"Next","tagName":"h2"},{"title":"Tailcall on AWS","type":0,"sectionRef":"#","url":"/docs/guides/tailcall-on-aws/","content":"","keywords":"","version":"Next"},{"title":"Logging​","type":1,"pageTitle":"Tailcall on AWS","url":"/docs/guides/tailcall-on-aws/#logging","content":" All Tailcall logs will be uploaded to and stored in AWS CloudWatch. Logs of all levels are stored by default, so that you can filter the logs as necessary when viewing them in CloudWatch.  If you would like to filter the logs before they get ingested, you can create the config/.env file and specify the minimum log level with the LOG_LEVEL environment variable. The available levels are: TRACE (default), DEBUG, INFO, WARN and ERROR. ","version":"Next","tagName":"h2"},{"title":"Scalars","type":0,"sectionRef":"#","url":"/docs/guides/scalar/","content":"","keywords":"","version":"Next"},{"title":"Default Scalars​","type":1,"pageTitle":"Scalars","url":"/docs/guides/scalar/#default-scalars","content":" Here is a list of default scalars that are built into the GraphQL Spec:  Scalar\tDescription\tSpecificationInt\tA type representing non-fractional signed whole numbers. Values can range up to (2^31 - 1).\tGraphQL Specification for Int Float\tA type for signed double-precision floating-point numbers.\tGraphQL Specification for Float String\tA sequence of UTF-8 characters, representing textual data.\tGraphQL Specification for String Boolean\tA boolean type that represents true or false.\tGraphQL Specification for Boolean ID\tA unique identifier, typically used to refetch an object or as a cache key.\tGraphQL Specification for ID  ","version":"Next","tagName":"h2"},{"title":"Tailcall Scalars​","type":1,"pageTitle":"Scalars","url":"/docs/guides/scalar/#tailcall-scalars","content":" These are the current set of custom scalars supported by Tailcall:  Scalar\tDescription\tSpecificationEmail\tA string that conforms to the email format as defined in the HTML specification, utilizing the Unicode character set.\tHTML Specification for Valid Email Addresses PhoneNumber\tA string format adhering to the E.164 international standard, which outlines the numbering plan for the worldwide public switched telephone network (PSTN) and certain data networks.\tE.164 International Numbering Plan Date\tA string that represents dates and times in the Internet protocols, following the ISO 8601 standard via the Gregorian calendar.\tRFC 3339 Date and Time Internet Formats Url\tA standardized format for Uniform Resource Identifiers (URI) that includes both the generic URI syntax and guidelines for resolving URI references, which may be in relative form.\tRFC 3986 Uniform Resource Identifier JSON\tA lightweight data interchange format based on the ECMAScript Programming Language Standard, designed for human-readable data representation.\tRFC 7159 The JavaScript Object Notation (JSON) Data Interchange Format Empty\tA type that represents no value or is used as a placeholder in contexts where no other data is expected or returned. It's equivalent to unit or void in other programming languages.\t  If none of the scalars make sense for your use case, consider opening an issue on the Tailcall github repository.  ","version":"Next","tagName":"h2"},{"title":"Example Usage​","type":1,"pageTitle":"Scalars","url":"/docs/guides/scalar/#example-usage","content":" Let's try using these custom scalars in our GraphQL schema.  schema @server( port: 8000 graphiql: true hostname: &quot;localhost&quot; ) { query: Query } type Query { email(value: Email!): Email! @expr(body: &quot;{{args.value}}&quot;) }   ","version":"Next","tagName":"h2"},{"title":"Valid Query Example​","type":1,"pageTitle":"Scalars","url":"/docs/guides/scalar/#valid-query-example","content":" Here is an example of a valid query that passes the custom scalar validations:  ","version":"Next","tagName":"h3"},{"title":"Invalid Query Example​","type":1,"pageTitle":"Scalars","url":"/docs/guides/scalar/#invalid-query-example","content":" And here is an example of an invalid query that fails the custom scalar validations as expected:  tip We recommend utilizing JSON as a scalar for cases where no other scalar type fits your needs. . ","version":"Next","tagName":"h3"},{"title":"GraphQL on gRPC","type":0,"sectionRef":"#","url":"/docs/guides/grpc/","content":"","keywords":"","version":"Next"},{"title":"What is gRPC?​","type":1,"pageTitle":"GraphQL on gRPC","url":"/docs/guides/grpc/#what-is-grpc","content":" This guide assumes a basic familiarity with gRPC. It is a high-performance framework created by Google for remote procedure calls (RPCs). Its key features include:  HTTP/2 Transport: Ensures efficient and fast data transfer.Protocol Buffers (Protobuf): Serves as a powerful interface description language.Efficiency: Offers binary serialization, reduces latency, and supports data streaming.  This combination of features makes gRPC ideal for microservices and distributed systems. If you need a more detailed understanding or are new to gRPC, we recommend visiting the official gRPC website for comprehensive documentation and resources.  Now, let's explore how gRPC can be integrated into our proxy gateway to enhance communication and data exchange in distributed systems.  ","version":"Next","tagName":"h2"},{"title":"gRPC upstream​","type":1,"pageTitle":"GraphQL on gRPC","url":"/docs/guides/grpc/#grpc-upstream","content":" We need some gRPC service available to be able to execute requests from the Tailcall gateway. For pure example purposes, we will build some simple gRPC services.  ","version":"Next","tagName":"h2"},{"title":"Protobuf definition​","type":1,"pageTitle":"GraphQL on gRPC","url":"/docs/guides/grpc/#protobuf-definition","content":" First, we need to create an example protobuf file that will define the structure of the data we want to transmit using gRPC. Here is the definition of NewsService that implements CRUD operations on news data that we'll put into the news.proto file.  syntax = &quot;proto3&quot;; import &quot;google/protobuf/empty.proto&quot;; package news; // Define message type for News with all its fields message News { int32 id = 1; string title = 2; string body = 3; string postImage = 4; } // Message with the id of a single news message NewsId { int32 id = 1; } // List of IDs of news to get multiple responses message MultipleNewsId { repeated NewsId ids = 1; } // List of all news message NewsList { repeated News news = 1; } // NewsService defines read and write operations for news items service NewsService { // GetAllNews retrieves all news items without any arguments rpc GetAllNews (google.protobuf.Empty) returns (NewsList) {} // GetNews fetches a single news item by its ID rpc GetNews (NewsId) returns (News) {} // GetMultipleNews retrieves multiple news items based on their IDs rpc GetMultipleNews (MultipleNewsId) returns (NewsList) {} }   ","version":"Next","tagName":"h3"},{"title":"Implement gRPC service​","type":1,"pageTitle":"GraphQL on gRPC","url":"/docs/guides/grpc/#implement-grpc-service","content":" Now having the protobuf file you can write a server that implements NewsService at any language you want that supports gRPC. Tailcall organization has a sample node.js service inside this repo that you can pull to your local machine. To spin up the sample service run inside the repo and wait for logs about the service running.  npm i npm start   ","version":"Next","tagName":"h3"},{"title":"Tailcall config​","type":1,"pageTitle":"GraphQL on gRPC","url":"/docs/guides/grpc/#tailcall-config","content":" Now when we have a running gRPC service we're going to write Tailcall's config to make the integration. To do this we need to specify GraphQL types corresponding to gRPC types we have defined in the protobuf file. Let's create a new file grpc.graphql file with the following content:  # The GraphQL representation for News message type type News { id: Int title: String body: String postImage: String } # Input type that is used to fetch news data by its id input NewsInput { id: Int } # Resolves multiple news entries type NewsData { news: [News]! }   Now when we have corresponding types in schema we want to define GraphQL Query that specifies the operation we can execute onto news. We can extend our config with the next Query:  type Query { # Get all news i.e. NewsService.GetAllNews news: NewsData! # Get single news by id i.e. NewsService.GetNews newsById(news: NewsInput!): News! }   Also, let's specify options for Tailcall's ingress and egress at the beginning of the config using @server and @upstream directives.  schema @server(port: 8000, graphiql: true) @upstream( baseURL: &quot;http://localhost:50051&quot; httpCache: true ) { query: Query }   To specify the protobuf file to read types from, use the @link directive with the type Protobuf on the schema. id is an important part of the definition that will be used by the @grpc directive later  schema @link(id: &quot;news&quot;, src: &quot;./news.proto&quot;, type: Protobuf)   Now you can connect GraphQL types to gRPC types. To do this you may want to explore more about @grpc directive. Its usage is pretty straightforward and requires you to specify the path to a method that should be used to make a call. The method name will start with the package name, followed by the service name and the method name, all separated by the . symbol.  If you need to provide any input to the gRPC method call you can specify it with the body option that allows you to specify a Mustache template and therefore it could use any input data like args and value to construct the body request. The body value is specified in the JSON format if you need to create the input manually and cannot use args input.  type Query { news: NewsData! @grpc(method: &quot;news.news.NewsService.GetAllNews&quot;) newsById(news: NewsInput!): News! @grpc( service: &quot;news.news.NewsService.GetNews&quot; body: &quot;{{args.news}}&quot; ) }   Wrapping up the whole result config that may look like this:  # file: app.graphql schema @server(port: 8000, graphiql: true) @upstream( baseURL: &quot;http://localhost:50051&quot; httpCache: true ) @link(id: &quot;news&quot;, src: &quot;./news.proto&quot;, type: Protobuf) { query: Query } type Query { news: NewsData! @grpc(method: &quot;news.news.NewsService.GetAllNews&quot;) newsById(news: NewsInput!): News! @grpc( method: &quot;news.news.NewsService.GetNews&quot; body: &quot;{{args.news}}&quot; ) } type News { id: Int title: String body: String postImage: String } input NewsInput { id: Int } type NewsData { news: [News]! }   Start the server by pointing it to the config.  tailcall start ./app.graphql   And now you can go to the page http://127.0.0.1:8000/graphql and execute some GraphQL queries e.g.:  { news { news { id title body } } }   Or  { newsById(news: {id: 2}) { id title body } }   ","version":"Next","tagName":"h2"},{"title":"Batching​","type":1,"pageTitle":"GraphQL on gRPC","url":"/docs/guides/grpc/#batching","content":" Another important feature of the @grpc directive is that it allows you to implement request batching for remote data almost effortlessly as soon as you have gRPC methods that resolve multiple responses for multiple inputs in a single request.  In our protobuf example file, we have a method called GetMultipleNews that we can use. To enable batching we need to enable @upstream.batch option first and specify batchKey option for the @grpc directive.  schema @server(port: 8000, graphiql: true) @upstream( baseURL: &quot;http://localhost:50051&quot; httpCache: true batch: {delay: 10} ) @link(id: &quot;news&quot;, src: &quot;./news.proto&quot;, type: Protobuf) { query: Query } type Query { newsById(news: NewsInput!): News! @grpc( method: &quot;news.NewsService.GetNews&quot; body: &quot;{{args.news}}&quot; batchKey: [&quot;news&quot;, &quot;id&quot;] ) }   Restart the Tailcall server and make the query with multiple news separately, e.g.:  { n1: newsById(news: {id: 1}) { id title body } n2: newsById(news: {id: 2}) { id title body } }   Those 2 requests will be executed inside a single request to the gRPC method GetMultipleNews  ","version":"Next","tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"GraphQL on gRPC","url":"/docs/guides/grpc/#conclusion","content":" Well done on integrating a gRPC service with the Tailcall gateway! This tutorial has demonstrated the straightforward and efficient process, showcasing Tailcall's compatibility with advanced communication protocols like gRPC.  You can find this working example and test it by yourself by the next links:  node-grpc - example implementation for gRPC service in node.jsgRPC example config - Tailcall's config to integrate with gRPC service above  ","version":"Next","tagName":"h2"},{"title":"Key Takeaways​","type":1,"pageTitle":"GraphQL on gRPC","url":"/docs/guides/grpc/#key-takeaways","content":" Simplicity of Integration: The integration of gRPC with Tailcall seamlessly enhances the overall capability of your system to handle high-performance and efficient data composition.Scalability and Performance: By leveraging the power of gRPC along with Tailcall, we've laid a foundation for building scalable and high-performing distributed systems.  ","version":"Next","tagName":"h3"},{"title":"Next Steps​","type":1,"pageTitle":"GraphQL on gRPC","url":"/docs/guides/grpc/#next-steps","content":" With the basics in place, we encourage you to explore further:  Dive Deeper: Tailcall gateway offers a lot of other features and configurations that you can utilize. Dive deeper into our documentation to explore more advanced settings and customization options.Explore Other Guides: Our documentation includes a variety of guides and tutorials that can help you leverage the full potential of Tailcall in different scenarios. Whether it's adding security layers, load balancing, or detailed logging, there's a lot more to explore. ","version":"Next","tagName":"h3"},{"title":"REST on top of GraphQL","type":0,"sectionRef":"#","url":"/docs/guides/rest/","content":"","keywords":"","version":"Next"},{"title":"How it works​","type":1,"pageTitle":"REST on top of GraphQL","url":"/docs/guides/rest/#how-it-works","content":"   Diagram showcasing how it works under hood  This guide show you how to expose REST endpoints for your GraphQL operations by using the @rest directive like follows:  There are three main steps to this process:  Define your Tailcall GraphQL configuration file.Define an operation using @rest directive in a separate file.Link the operation to the main config file.  ","version":"Next","tagName":"h2"},{"title":"Example​","type":1,"pageTitle":"REST on top of GraphQL","url":"/docs/guides/rest/#example","content":" ","version":"Next","tagName":"h2"},{"title":"Step 1: Define your Tailcall GraphQL configuration​","type":1,"pageTitle":"REST on top of GraphQL","url":"/docs/guides/rest/#step-1-define-your-tailcall-graphql-configuration","content":" schema @server(graphiql: true) @upstream( baseURL: &quot;https://jsonplaceholder.typicode.com&quot; ) { query: Query } type Query { post(id: Int!): Post @http(path: &quot;/posts/{{args.id}}&quot;) } type Post { userId: Int! id: Int title: String body: String user: User @http(path: &quot;/users/{{value.userId}}&quot;) } type User { id: Int! name: String! username: String! email: String! phone: String website: String }   for more information on how to define your Tailcall GraphQL configuration file, please refer to the Tailcall GraphQL Configuration.  ","version":"Next","tagName":"h3"},{"title":"Step 2: Define an operation using @rest directive​","type":1,"pageTitle":"REST on top of GraphQL","url":"/docs/guides/rest/#step-2-define-an-operation-using-rest-directive","content":" We will define an operation and use the @rest directive to define a REST endpoint for the operation. We will create a new file and add the following content to it. Save the file with the filename: user-operation.graphql. You can name the file anything you want, but make sure to link it to the main config file in the next step.  query ($id: Int!) @rest(method: GET, path: &quot;/post/$id&quot;) { post(id: $id) { id title body user { id name } } }   to know more about the @rest directive, please refer to the Tailcall GraphQL Directives.  ","version":"Next","tagName":"h3"},{"title":"Step 3: Link the operation to the main config file​","type":1,"pageTitle":"REST on top of GraphQL","url":"/docs/guides/rest/#step-3-link-the-operation-to-the-main-config-file","content":" checkout the @link directive in the config snippet below to link the operation file. This step is crucial to make the REST endpoint available.  schema @server(graphiql: true) @upstream(baseURL: &quot;https://jsonplaceholder.typicode.com&quot;) @link(type: Operation, src: &quot;user-operation.graphql&quot;) { query: Query }   To know more about the @link directive, please refer to the Tailcall GraphQL Directives.  Response​    In summary, by utilizing the @rest directive, we've seamlessly exposed RESTful services over Tailcall's GraphQL, enhancing the traditional posts API to offer richer functionality without additional code. This approach combines the simplicity and ubiquity of REST with the modularity and flexibility of GraphQL, allowing for easy consumption from any HTTP client while leveraging GraphQL's powerful data querying capabilities. ","version":"Next","tagName":"h3"},{"title":"Watch Mode","type":0,"sectionRef":"#","url":"/docs/guides/watch-mode/","content":"","keywords":"","version":"Next"},{"title":"Use case​","type":1,"pageTitle":"Watch Mode","url":"/docs/guides/watch-mode/#use-case","content":" Running a server in watch mode offers a lot of key benefits:  Real-time Feedback: Watch mode ensures that your server remains up-to-date with your code changes, instantly reflecting those changes and providing you with real-time feedback during development.Efficiency: Manually restarting the server each time you change code can be tedious and time-consuming. Watch mode automates this process, enhancing development efficiency.Debugging: It enables you to identify and resolve issues as they occur, reducing debugging time. With your server automatically restarting upon code changes, you detect errors earlier.  ","version":"Next","tagName":"h2"},{"title":"Using entr​","type":1,"pageTitle":"Watch Mode","url":"/docs/guides/watch-mode/#using-entr","content":" It's a powerful file-watching utility that makes running a server in watch mode a breeze. Let's go through the steps for the installation process for different operating system :  ","version":"Next","tagName":"h2"},{"title":"Installation​","type":1,"pageTitle":"Watch Mode","url":"/docs/guides/watch-mode/#installation","content":" Homebrew​  Open the Terminal, which you can find in the &quot;Utilities&quot; folder within the &quot;Applications&quot; folder. Install Homebrew if you haven't already. Run the following command in your Terminal: /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)&quot; After installing Homebrew, proceed to install entr by executing the following command: brew install entr To verify the installation, run: entr --version   Upon successful installation, it will display the latest version of entr.  Windows Subsystem​  Install Windows Subsystem for Linux (WSL) on your Windows machine by following Microsoft's official documentation. After setting up WSL, open the Linux terminal by running: wsl -d &lt;DistributionName&gt; Replace &lt;DistributionName&gt; with the name of the Linux distribution that you have installed. Install entr within the Linux terminal using the package manager of your chosen Linux distribution. For example, on Ubuntu, you can use: sudo apt update sudo apt install entr Verify the installation by running: entr --version   A successful installation will display the latest version of entr.  apt-get​  On Linux, you can install entr using your distribution's package manager. For example, on Ubuntu, use: sudo apt update sudo apt install entr To verify the installation, run: entr --version   If you install it, it will show the latest version of the entr  ","version":"Next","tagName":"h3"},{"title":"Watch Mode​","type":1,"pageTitle":"Watch Mode","url":"/docs/guides/watch-mode/#watch-mode","content":" To run your server in watch mode with entr, use the ls command to list the files you want to track. The general syntax is as follows:  ls *.graphql | entr -r tailcall start ./jsonplaceholder.graphql   This command uses entr to continuously track the jsonplaceholder.graphql file and when it changes, It runs the tailcall start command with the file as an argument  Detailing the above command as follows:  ls *.graphql : This part of the code lists the file or files you want to track for changes. In this case, it lists the file named &quot;jsonplaceholder.graphql&quot; within the &quot;examples&quot; directory. | : The pipe symbol ('|') takes the output of the preceding command (the file listing) and feeds it as input to the following command (entr). entr -r tc start ./jsonplaceholder.graphql : Whenever the file &quot;jsonplaceholder.graphql&quot; changes, this command executes.  entr is a command-line tool for running arbitrary commands whenever files change. It tracks the files specified in the previous command (ls ./jsonplaceholder.graphql) r : This flag instructs entr to persist in running the command through errors, ensuring continuous operation. tc start ./jsonplaceholder.graphql : This command runs upon detecting changes, executing tc start with the file path./jsonplaceholder.graphql as an argument  ","version":"Next","tagName":"h3"},{"title":"Some Best Practices​","type":1,"pageTitle":"Watch Mode","url":"/docs/guides/watch-mode/#some-best-practices","content":" To make the most of running a server in watch mode with entr, consider the following best practices:  Selective File Watching: Be selective about which files you track with entr. Watching unnecessary files can lead to increased CPU and memory usage. Focus on the essential files related to your project. Organize Your Project: Maintain a well-organized project structure to make it easier to identify which files need tracking. Clear Output: Clear the terminal output before running entr to have a clean workspace. Version Control: Ensure that your project is under version control (e.g., Git) to track changes and revert if necessary. Update entr: Ensure entr is always updated to the latest version for bug fixes and enhancements.  By following these best practices and using entr effectively, you can greatly improve your development workflow. Experiment with entr, adapt it to your project's specific requirements, and enjoy a smoother and more efficient development process. Happy coding! ","version":"Next","tagName":"h2"},{"title":"Apollo Studio","type":0,"sectionRef":"#","url":"/docs/telemetry/apollo-studio/","content":"","keywords":"","version":"Next"},{"title":"Creating a monolith graph​","type":1,"pageTitle":"Apollo Studio","url":"/docs/telemetry/apollo-studio/#creating-a-monolith-graph","content":" Before you configure tailcall, you will need to create a Monolith graph on Apollo Studio. Go to your organization's home page and click on Create your first graph, if this is your first graph or Create New Graph if you have existing graphs. Change the Graph title, Graph ID and other fields as desired and make sure to change Graph Architecture to Monolith, assuming tailcall is booted in monolith mode. Once you are done, click on Next. You'll see the following screen. Copy the fields APOLLO_KEY and APOLLO_GRAPH_REF as they are required by tailcall to be able to send the usage metrics. Next we need to connect Apollo with our running instance of Tailcall. There are two ways to let Apollo know about your GraphQL schema: Navigate to Local Introspection. If you have a deployed instance of your GraphQL server you can put the URL pointing to that in Endpoint URL and click on Introspect and Upload. If not, start a local instance of tailcall and put the local url here, similar to how is shown in the image below. You can start a local instance of Tailcall by running tailcall start (click here to know more). Or, Navigate to Local Schema and insert your schema generated by tailcall and click Upload. You can get the schema by running tailcall check (click here to know more).  You have now created a Monolith graph in Apollo Studio. The next step is to configure tailcall to use the APOLLO_API_KEY and APOLLO_GRAPH_REF. Follow detailed instructions here.  ","version":"Next","tagName":"h2"},{"title":"Checking the metrics in Apollo Studio​","type":1,"pageTitle":"Apollo Studio","url":"/docs/telemetry/apollo-studio/#checking-the-metrics-in-apollo-studio","content":" To see the metrics for you queries follow these instructions:  Start tailcall with the appropriate configuration for Apollo (click here to know more). Below is an example of what a config may look like: schema @server(port: 8000, graphiql: true) @upstream( baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) @telemetry( export: { apollo: { apiKey: &quot;&lt;APOLLO_API_KEY from Apollo Website&gt;&quot; graphRef: &quot;&lt;APOLLO_GRAPH_REF from Apollo Website&gt;&quot; } } ) { query: Query } type Query { posts: [Post] @http(path: &quot;/posts&quot;) } type Post { id: Int! userId: Int! title: String! body: String! } Visit http://localhost:8000/graphql and create a query with an appropriate name (below is an example query named MyQuery) and run it multiple times to send the metrics to Apollo Studio. tip Naming the query is not required to be able to send the metrics, but it helps to organize the metrics with appropriate names when viewed in Apollo Studio. query MyQuery { posts { id title } } To see the metrics click on the VARIANT NAME of your graph. In the example below, the variant name is current. You will see the following page. From here click on insights icon as highlighted on the left side of the image. You will now be able to see all the information related to your queries here  important If you don't see the name of your query here, try running the query multiple times and waiting for some time. Since the metric isn't sent to Apollo Studio for each query, instead they are batched together and sent at once for efficiency reasons. ","version":"Next","tagName":"h2"},{"title":"Data Dog","type":0,"sectionRef":"#","url":"/docs/telemetry/data-dog/","content":"Data Dog This guide is based on the official doc. Go to datadoghq.comLogin to your account (make sure you choose right region for your account on login)Go to Organization Settings -&gt; API Keys and copy the value of existing key or create a new oneIntegration with datadog requires OpenTelemetry Collector to be able to send data to. As an example we can use following config for the collector: receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 exporters: logging: verbosity: detailed datadog: traces: span_name_as_resource_name: true hostname: &quot;otelcol&quot; api: key: ${DATADOG_API_KEY} # make sure to specify right datadog site based on # https://docs.datadoghq.com/getting_started/site/ site: us5.datadoghq.com processors: batch: datadog/processor: probabilistic_sampler: sampling_percentage: 30 service: pipelines: traces: receivers: [otlp] processors: [batch, datadog/processor] exporters: [datadog] metrics: receivers: [otlp] processors: [batch] exporters: [datadog] logs: receivers: [otlp] processors: [batch] exporters: [datadog] Go to your tailcall config and update it to: schema @telemetry( export: {otlp: {url: &quot;http://localhost:4317&quot;}} ) { query: Query } Set the api key you've copied before to the environment variable named DATADOG_API_KEY and start Otel collector and tailcall with updated config Now make some requests to running service and wait a little bit until Datadog proceeds the data. After that you can go to APM -&gt; Traces, locate the span with name request and click on it. You should see something like on screenshot below: To see metrics now go to Metrics -&gt; Explorer and search for metric you want to see. After updating the query you should see something like on example below:","keywords":"","version":"Next"},{"title":"Honeycomb","type":0,"sectionRef":"#","url":"/docs/telemetry/honey-comb/","content":"Honeycomb Go to honeycomb.ioLogin to your accountGo to Account -&gt; Team Settings -&gt; Environments and API Keys -&gt; Configuration and create new or copy existing api keyGo to tailcall config and update settings: schema @telemetry( export: { otlp: { url: &quot;https://api.honeycomb.io:443&quot; headers: [ { key: &quot;x-honeycomb-team&quot; value: &quot;{{env.HONEYCOMB_API_KEY}}&quot; } { key: &quot;x-honeycomb-dataset&quot; value: &quot;&lt;your-dataset&gt;&quot; } ] } } ) { query: Query } Set the api key you've copied before to the environment variable named HONEYCOMB_API_KEY and start tailcall with updated config Now make some requests to running service and wait a little bit until honeycomb proceeds the data. After that you can go to Home -&gt; Total traces and click on the trace with name request. Now choose Traces in the bottom and click on the first trace from the list. You should see the picture similar to this: Here you can see data about the request that was made to tailcall and what actions were made to handle this request. To see metrics now go Query and run a query to fetch the data about metrics. You can use following screenshot as an example:","keywords":"","version":"Next"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/docs/telemetry/","content":"","keywords":"","version":"Next"},{"title":"What is Observability​","type":1,"pageTitle":"Introduction","url":"/docs/telemetry/#what-is-observability","content":" Observability is essential for maintaining the health and performance of your applications. It provides insights into your software's operation in real-time by analyzing telemetry data — logs, metrics, and traces. This data helps in troubleshooting, optimizing, and ensuring your application works as expected.  Logs offer a record of events that have happened within your application, useful for understanding actions taken or errors that have occurred.Metrics are numerical data that measure different aspects of your system's performance, such as request rates or memory usage.Traces show the journey of requests through your system, highlighting how different parts of your application interact and perform.  Tailcall provides observability support by integrating OpenTelemetry specification into it with help of provided SDKs and data formats.  OpenTelemetry is a toolkit for collecting telemetry data in a consistent manner across different languages and platforms. It frees you from being locked into a single observability platform, allowing you to send your data to different tools for analysis, such as New Relic or Honeycomb.  ","version":"Next","tagName":"h2"},{"title":"Comparison with Apollo Studio​","type":1,"pageTitle":"Introduction","url":"/docs/telemetry/#comparison-with-apollo-studio","content":" While Apollo studio telemetry also provides analytics tools for your schema but when choosing between it and OpenTelemetry integration consider next points:  OpenTelemetry is more generalized observability framework that could be used for cross-service analytics while Apollo Studio can provide insights related purely to graphQLOpenTelemetry is vendor-agnostic and therefore you could actually use different observability platforms depending on your needs and don't rely on single tool like Apollo StudioOpenTelemetry integration in Tailcall can provide more analytical data that is out of scope of graphQL analytics provided by Apollo Studio  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Introduction","url":"/docs/telemetry/#prerequisites","content":" Consider we have following Tailcall config that connects with jsonplaceholder.com to fetch the data about user and posts  schema @server(port: 8000, graphiql: true, hostname: &quot;0.0.0.0&quot;) @upstream( baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query } type Query { posts: [Post] @http(path: &quot;/posts&quot;) @cache(maxAge: 3000) user(id: Int!): User @http(path: &quot;/users/{{args.id}}&quot;) } type User { id: Int! name: String! username: String! email: String! phone: String website: String } type Post { id: Int! userId: Int! title: String! body: String! user: User @http(path: &quot;/users/{{value.userId}}&quot;) }   We will update that config with telemetry integration in following sections.  ","version":"Next","tagName":"h2"},{"title":"Tailcall config​","type":1,"pageTitle":"Introduction","url":"/docs/telemetry/#tailcall-config","content":" By default, telemetry data is not generated by Tailcall since it requires some setup to know where to send this data and also that affects performance of server that could be undesirable in some cases.  Telemetry configuration is provided by @telemetry directive to setup how and where the telemetry data is send.  To enable it we can update our config with something like config below:  schema @telemetry( export: { otlp: {url: &quot;http://your-otlp-compatible-backend.com&quot;} } ) { query: Query }   Here, export specifies the format of generated data and endpoint to which to send that data. Continue reading to know more about different options for it.  ","version":"Next","tagName":"h2"},{"title":"Export to OTLP​","type":1,"pageTitle":"Introduction","url":"/docs/telemetry/#export-to-otlp","content":" OTLP is a vendor agnostic protocol that is supported by growing number of observability backends.  OpenTelemetry Collector​  OpenTelemetry Collector is a vendor-agnostic way to receive, process and export telemetry data in OTLP format.  Although, tailcall can send the data directly to the backends that supports OTLP format using Otel Collector could be valuable choice since it's more robust solution well-suited for a high-scale, more flexible settings and ability to export in different formats other than OTLP.  In summary, if you're gonna to use OTLP compatible platform or prometheus and your load is not that massive you could send the data directly to platforms. From the other side, if you need to export to different formats (like Jaeger or Datadog) or your application involves high load consider using Otel Collector as an export target.  ","version":"Next","tagName":"h3"},{"title":"Export to prometheus​","type":1,"pageTitle":"Introduction","url":"/docs/telemetry/#export-to-prometheus","content":" Prometheus is a metric monitoring solution. Please note that prometheus works purely with metrics and other telemetry data like traces and logs won't be sent to it.  Prometheus integration works by adding a special route for tailcall server router that outputs generated metrics in prometheus format consumable by prometheus scraper.  ","version":"Next","tagName":"h3"},{"title":"Data generated​","type":1,"pageTitle":"Introduction","url":"/docs/telemetry/#data-generated","content":" You can find a reference of type of info generated by Tailcall in the @telemetry reference or consult examples in the next section, in order to gain some understanding.  ","version":"Next","tagName":"h2"},{"title":"Relation with other services​","type":1,"pageTitle":"Introduction","url":"/docs/telemetry/#relation-with-other-services","content":" Tailcall fully supports Context Propagation functionality and therefore you can analyze distributed traces across all of your services that are provides telemetry data.  That may look like this:    Where Tailcall is a part of whole distributed trace  ","version":"Next","tagName":"h3"},{"title":"Customize generated data​","type":1,"pageTitle":"Introduction","url":"/docs/telemetry/#customize-generated-data","content":" In some cases you may want to customize the data that were added to telemetry payload to have more control over analyzing process. Tailcall supports that customization for specific use cases described below. For eg. the metric http.server.request.count can be customized with the requestHeaders property to allow splitting the overall count by specific headers.  important The value of specified headers will be sent to telemetry backend as is, so use it with care to prevent of leaking any sensitive data to third-party services you don't have control over. ","version":"Next","tagName":"h3"},{"title":"New Relic","type":0,"sectionRef":"#","url":"/docs/telemetry/new-relic/","content":"New Relic The guide is based on official doc Go to newrelic.comLogin to your accountGo to &lt;your user name&gt; -&gt; Api Keys and copy license value for key with access to write dataGo to tailcall config and update it with: schema @telemetry( export: { otlp: { url: &quot;https://otlp.nr-data.net:4317&quot; headers: [ { key: &quot;api-key&quot; value: &quot;{{env.NEWRELIC_API_KEY}}&quot; } ] } } ) { query: Query } Set the api key you've copied before to the environment variable named NEWRELIC_API_KEY and start tailcall with updated config Now make some requests to running service and wait a little bit until New Relic proceeds the data. After that you can go to Traces locate request trace, click on it, then pick one of the available traces and click on it. You should see something like the screenshot below: To see metrics now go to APM &amp; Services -&gt; Metrics Explorer and choose the metrics you want to see like on example below.","keywords":"","version":"Next"}],"options":{"highlightResult":true,"id":"default"}}