---
title: "Markdown-based tests"
description: "This guide details Tailcall's Markdown-based snapshot testing framework, explaining its structure, syntax, and testing process. It is designed for developers implementing or optimizing tests in a language-agnostic environment, emphasizing efficient snapshot utilization and maintenance within Tailcall."
sidebar_position: 4
---

A Markdown-based snapshot testing framework in **Tailcall**.

## Table of contents

- [Why a new testing framework?](#why-a-new-testing-framework)
- [How does it work?](#how-does-it-work)
- [Run tests](#run-tests)
  - [Filter tests](#filter-tests)
- [Structure](#structure)
- [Test syntax](#test-syntax)
  - [Header](#header)
  - [Blocks](#blocks)
    - [`@server`](#server)
    - [`@mock`](#mock)
    - [`@env`](#env)
    - [`@test`](#test)
    - [`@file:<filename>`](#filefilename)
  - [Instruction](#instruction)
- [Test process](#test-process)
- [Snapshots](#snapshots)
- [Maintenance](#maintenance)

## Why a new testing framework?

We aimed to create a snapshot testing framework that is language-agnostic, straightforward to write, maintain, and understand. For this reason, we chose a Markdown-based design. This design aligns with the usage patterns of Tailcall users. Since Tailcall supports building scalable GraphQL backends without being tied to a specific programming language, it was essential for our testing framework to be similarly language-agnostic.

## How does it work?

[execution_spec](https://github.com/tailcallhq/tailcall/blob/main/tests/execution_spec.rs) implements a Markdown-based snapshot testing framework for Tailcall. The framework is designed to test the execution of Tailcall configs, and it is based on the following architecture:
![Test Architecture](../../static/images/contributors/test-arch.png)

## Run tests

The markdown-based tests are executed as usual integration test so you can use test options and filters like with usual test.

To run markdown-based tests skipping other tests run following command:

```sh
cargo test --test execution_spec
```

After running you will get an output of all executed markdown tests.

### Filter tests

If you want to run specific set of tests you have two options.

First, if you want to filter out tests when you run it locally then you can use [testing filters](./testing.md#filter-running-tests). For example, to filter grpc related tests:

```sh
# to run grpc tests
cargo test --test execution_spec grpc
# or to skip grpc tests
cargo test --test execution_spec -- --skip grpc
```

Second, if you want to disable some tests permanently due to one of the following reasons:

- Some functionalities which are supposed to be tested might not be implemented yet
- There might be some bugs related to it.

In that case you can a special annotation `##### skip` that should be placed inside the test md file. For example:

```md
# Failing test

##### skip

  <!-- Some test content -->
```

This test won't be executed until the annotation is removed.

## Structure

All `execution_spec` tests are located in `tests/execution/`. The results generated by these tests are stored as snapshots in `tests/snapshots/`.

## Test syntax

An `execution_spec` test is always a Markdown file with a `.md` extension. These files contain the following parts:

### Header

A level 1 heading (`#`) specifying the name of the test, and an optional paragraph after it specifying a description. There must be precisely one header in a test.

Examples:

- ```md
  # Simple test
  ```
- ```md
  # Complex test

  This is a description.
  ```

### Blocks

Blocks are specified along with the codeblocks next to the format of the codeblock (`@`) followed by the block type, and a code block after them. Blocks supply the runner with data, and the runner determines what to do based on the available blocks. example:

````md {1}
```graphql @server
schema {
  query: Query
}
```
````

#### `@server`

A `@server` block lets you specify a server SDL config. These are expected to be parseable to have a passing test, unless the [`SDL error` instruction](#instruction) is specified, which requires the config parsing to throw an error. There must be at least one `@server` block in a test.

Every test should have at least one `@server` block. Some blocks (for example, `@test`) require precisely one `@server` block. Moreover, having precisely one `@server` block in a test ensures that the `client` check will also be performed.

The `merge` check is always performed using every defined server block.

When the [`check identity` instruction](#instruction) is specified, the runner will attempt to perform an `identity` check, but since is a "dumb", plain-text check, it requires the `server` block's code to be written in a specific way.

Example:

````md {1}
```graphql @server
schema {
  query: Query
}

type User {
  id: Int
  name: String
}

type Query {
  user: User
    @http(
      path: "/users/1"
      baseURL: "http://jsonplaceholder.typicode.com"
    )
}
```
````

#### `@mock`

A `@mock` block specifies mocked HTTP endpoints in `YAML`.

An item of `mock` contains a `request` and a `response`.

There may be at most one `mock` block in a test.

Example:

````md {1}
```graphql @mock
- request:
    method: GET
    url: http://jsonplaceholder.typicode.com/users/1
  response:
    status: 200
    body:
      id: 1
      name: foo
```
````

#### `@env`

An `@env` block specifies environment variables in `YAML` that the runner should use in the app context.

There may be at most one `@env` block in a test.

Example:

````md {1}
```yml @env
TEST_ID: 1
```
````

#### `@test`

An `@test` block specifies HTTP requests that the runner should perform in `YAML`.
It solely contains requests. The response for each request is stored in an `test_{i}` snapshot.

There may be at most one `@test` block in a test.

Example:

````md {1}
```graphql @test
- method: POST
  url: http://localhost:8080/graphql
  body:
    query: query { user { name } }
```
````

#### `@file:<filename>`

A `@file` block creates a file in the spec's virtual file system. The [`@server` block](#server) will have exclusive access to files created in this way: the true filesystem is not available to it.

Every `@file` block has the filename declared in the header. The language of the code block is optional and does not matter.

Example:

````md {1}
```graphql @file:enum.graphql
schema
  @server(port: 8080)
  @upstream(
    baseURL: "http://jsonplaceholder.typicode.com"
  ) {
  query: Query
}

enum Foo {
  BAR
  BAZ
}

type Query {
  foo: Foo @http(path: "/foo")
}
```
````

### Instruction

A header (`---`), followed by an instruction:

```md
---
expect_validation_error: true
---
```

- This instructs the runner to expect a failure when parsing the `server` block and to compare the result with an `errors` snapshot. This is used when testing for error handling.

```md
---
check_identity: true
---
```

- This instructs the runner to run identity checks on `server` blocks. While it would be good to run this on every test, the code of `server` blocks must be written with this instruction mind, therefore it is optional.

There must be precisely zero or one instruction in a test.

## Test process

1. The runner reads all tests, and selects the ones to run based on the following:
   - If a path to a test was given in the first command line argument, solely that test will run.
   - If one or more tests have a [`skip` annotation](#annotation), every test except those will run.
   - If none of the above is true, all tests will run.
2. The runner evaluates every test.
   1. If the test has an [`SDL error` instruction](#instruction), the runner does the following:
      1. Reads and parses the config, taking note of the validation errors.
      2. **If no validation errors occurred, the runner throws an error.** (`SDL error` is a requirement, not a try-catch.)
      3. Compares the encountered errors to the `errors` snapshot.
      4. If the snapshot doesn't match the encountered errors, the runner generates a new snapshot and throws an error.
      5. Ends the test run, and starts evaluating the next test. (All other actions would require a parseable `@server` block.)
      6. The runner parses every `@server` block.
   2. Parses the block and checks for errors.
   3. If the test has a [`check identity` instruction](#instruction), the runner converts the parsed block to SDL again, and checks if the two strings are the same. If they're not, the runner throws an error.
   4. The runner performs a `merge` check:
      1. Attempts to merge all [`@server` blocks](#server), resulting in a merged config. (If there is a single [`@server` block](#server), the runner will merge it with the default config.)
      2. Compares the merged config to the `merged` snapshot.
      3. If the snapshot doesn't match the merged config, the runner generates a new snapshot and throws an error.
   5. If there is precisely one [`@server` block](#server), the runner performs a `client` check:
      1. Generates the client schema of the `server` block.
      2. Compares it to the `client` SDL snapshot.
      3. If the snapshot doesn't match the generated schema, the runner generates a new snapshot and throws an error.
   6. If the test has an [`@test` block](#test), the runner performs `test` checks:
      1. If there is a [`@mock` block](#mock), the runner sets up the mock HTTP client based on it.
      2. If there is at least one [`@file` block](#filefilename), the runner sets up the mock filesystem based on them.
      3. If there is an [`@env` block](#env), the runner uses it for the app context.
      4. Creates an app context based on the [`@server` block](#server).
      5. For each test in the block (0-based index `i`), the runner does the following:
         1. Runs the HTTP request on the app context.
         2. Compares the HTTP response to the `test_{i}` snapshot.
         3. If the snapshot doesn't match the response, the runner generates a new snapshot and throws an error.

## Snapshots

`execution_spec` uses the [Insta](https://insta.rs) snapshot engine. Snapshots are automatically generated with a `.new` suffix if there is no pre-existing snapshot, or if the compared data didn't match the existing snapshot.

Instead of writing result cases in tests and updating them when behaviour changes, a snapshot-based testing workflow relies on auto-generation. Whenever a `.new` snapshot is generated, it means one of the following:

- Your code made an unexpected breaking change, and you need to fix it.
- Your code made an expected breaking change, and you need to accept the new snapshot.

You need to determine which one is the case, and take action accordingly.

Usage of [cargo-insta](https://insta.rs/docs/cli/) is recommended:

```bash
cargo insta test --review
```

This will regenerate all snapshots without interrupting the test every time there's a diff, and it will also open the snapshot review interface, so that you can accept or reject `.new` snapshots.

## Maintenance

1. To clean unused snapshots, run `cargo insta test --delete-unreferenced-snapshots`.
