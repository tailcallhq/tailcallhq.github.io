---
title: "Integration Testing"
description: "Discover Tailcall's innovative markdown-based snapshot testing framework, designed to enhance testing across various programming languages seamlessly. This framework focuses on simplicity and maintainability, utilizing Markdown to make test cases easy to write and understand. Tailored for users of Tailcall, the framework supports extensive testing of GraphQL backends without language constraints. Features include detailed instructions on running and filtering tests, a structured test syntax guide, and comprehensive sections on test processes and snapshot maintenance. Ideal for developers seeking a straightforward, language-agnostic testing solution. Learn more about implementing and maintaining effective tests with Tailcall's unique approach."
sidebar_position: 4
---

import CodeBlock from "@theme/CodeBlock"

As you may be aware, Tailcall offers a method for writing a [configuration](../docs/guides/configuration.mdx) to generate a GraphQL backend. Additionally, you can [link](../docs/directives/link.md) multiple configurations to compose them together. Extending the behavior of Tailcall is also possible by integrating custom JavaScript [scripts](../docs/guides/http-filters.md). Managing this involves handling multiple files in various formats, which complicates the experience of writing integration tests.

To maintain control, we have opted to utilize markdown files, allowing us to consolidate various types of configurations and scripts into a single document.

Here is an example of how the test looks:

<CodeBlock language="md">
{`
---
identity: true
---

\`\`\`graphql
schema @upstream(baseURL: "http://jsonplaceholder.typicode.com") {
query: Query
}

type Query {
post: Post @http(path: "/post")
}

type Post {
id: Int
title: String
body: String
}
\`\`\`
`.trim()}

</CodeBlock>

## How does it work?

[Execution Spec](https://github.com/tailcallhq/tailcall/blob/main/tests/execution_spec.rs) implements a custom markdown-based testing framework for Tailcall. The framework is designed to help write integration tests for Tailcall configs.

### Run all tests

The integration tests are executed as usual integration test so you can use test options and filters like with usual test.

```sh
cargo test
```

To run integration tests skipping other tests run following command:

```sh
cargo test --test execution_spec
```

After running you will get an output of all executed integration tests.

### Run a single test

Similar to [filtering unit tests](./testing.md#filtering-running-tests) to execute a single markdown configuration you can pass it's name to the test command:

```sh
cargo test --test execution_spec grpc
   Compiling tailcall-fixtures v0.1.0 (/Users/tushar/Documents/Projects/tailcall/tailcall-fixtures)
   Compiling tailcall v0.1.0 (/Users/tushar/Documents/Projects/tailcall)
    Finished `test` profile [unoptimized + debuginfo] target(s) in 15.96s
     Running tests/execution_spec.rs (target/debug/deps/execution_spec-6779d7c5c29b9b0b)

running 18 tests
test run_execution_spec::test-grpc-invalid-method-format.md ... ok
test run_execution_spec::test-grpc-invalid-proto-id.md      ... ok
test run_execution_spec::test-grpc-group-by.md              ... ok
test run_execution_spec::test-grpc-missing-fields.md        ... ok
test run_execution_spec::test-grpc-nested-optional.md       ... ok
test run_execution_spec::test-grpc-nested-data.md           ... ok
test run_execution_spec::test-grpc-proto-path.md            ... ok
test run_execution_spec::grpc-proto-with-same-package.md    ... ok
test run_execution_spec::grpc-reflection.md                 ... ok
test run_execution_spec::test-grpc-optional.md              ... ok
test run_execution_spec::test-grpc-service-method.md        ... ok
test run_execution_spec::test-grpc-service.md               ... ok
test run_execution_spec::grpc-error.md                      ... ok
test run_execution_spec::grpc-simple.md                     ... ok
test run_execution_spec::grpc-batch.md                      ... ok
test run_execution_spec::grpc-url-from-upstream.md          ... ok
test run_execution_spec::grpc-override-url-from-upstream.md ... ok
test run_execution_spec::test-grpc.md                       ... ok
```

In the above command all tests with the name `grpc` will be executed.

### Skipping a test

Skipping the test is also possible by passing the `--skip` parameter:

```sh
cargo test --test execution_spec -- --skip grpc
```

Sometimes, you might want to skip the test per permanently for everyone and the CI. You could achieve it by setting the `skip` configuration in your markdown:

```md
---
skip: true
---

<!-- Rest of the configurations -->
```

## Folder Structure

All `execution_spec` tests are located in [tests/execution](https://github.com/tailcallhq/tailcall/tree/main/tests/execution). The results generated by these tests are stored as snapshots in [tests/snapshots](https://github.com/tailcallhq/tailcall/tree/main/tests/execution). An `execution_spec` test is always a markdown file with a `.md` extension.

## File Structure

Each `.md` file runs in its own scope, so no two tests can interfere with each other. The file structure is as follows:

### Heading

The heading of file structure is used to provide metadata about the test. It is a YAML front matter block that contains the following fields:

- `identity` - This instructs the runner to check if the configuration when parsed and then printed back, is the same as the original configuration. This is useful to check whenever a new feature is added in the configuration and the parsers + printer needs to be updated.
- `error` - This instructs the runner to expect a validation error while parsing the configuration. This is useful to test validation logic written while converting config to blueprint.
- `skip` - This is a special annotation that ensures that the test is skipped.

<CodeBlock language="md">
  {`
---
identity: true
error: true
skip: true
---
`.trim()}
</CodeBlock>

### Code Blocks

Blocks are specified along with the codeblocks next to the format of the codeblock (`@`) followed by the block type, and a code block after them. Blocks supply the runner with data, and the runner determines what to do based on the available blocks. example:

````md {1}
```graphql @server
schema {
  query: Query
}
```
````

#### `@server`

A `@server` block lets you specify a server SDL config. These are expected to be parseable to have a passing test, unless the [`SDL error` instruction](#instruction) is specified, which requires the config parsing to throw an error. There must be at least one `@server` block in a test.

Every test should have at least one `@server` block. Some blocks (for example, `@test`) require precisely one `@server` block. Moreover, having precisely one `@server` block in a test ensures that the `client` check will also be performed.

The `merge` check is always performed using every defined server block.

When the [`check identity` instruction](#instruction) is specified, the runner will attempt to perform an `identity` check, but since is a "dumb", plain-text check, it requires the `server` block's code to be written in a specific way.

Example:

````md {1}
```graphql @server
schema {
  query: Query
}

type User {
  id: Int
  name: String
}

type Query {
  user: User @http(path: "/users/1", baseURL: "http://jsonplaceholder.typicode.com")
}
```
````

#### `@mock`

A `@mock` block specifies mocked HTTP endpoints in `YAML`.

An item of `mock` contains a `request` and a `response`.

There may be at most one `mock` block in a test.

Example:

````md {1}
```graphql @mock
- request:
    method: GET
    url: http://jsonplaceholder.typicode.com/users/1
  response:
    status: 200
    body:
      id: 1
      name: foo
```
````

#### `@env`

An `@env` block specifies environment variables in `YAML` that the runner should use in the app context.

There may be at most one `@env` block in a test.

Example:

````md {1}
```yml @env
TEST_ID: 1
```
````

#### `@test`

An `@test` block specifies HTTP requests that the runner should perform in `YAML`.
It solely contains requests. The response for each request is stored in an `test_{i}` snapshot.

There may be at most one `@test` block in a test.

Example:

````md {1}
```graphql @test
- method: POST
  url: http://localhost:8080/graphql
  body:
    query: query { user { name } }
```
````

#### `@file:<filename>`

A `@file` block creates a file in the spec's virtual file system. The [`@server` block](#server) will have exclusive access to files created in this way: the true filesystem is not available to it.

Every `@file` block has the filename declared in the header. The language of the code block is optional and does not matter.

Example:

````md {1}
```graphql @file:enum.graphql
schema @server(port: 8080) @upstream(baseURL: "http://jsonplaceholder.typicode.com") {
  query: Query
}

enum Foo {
  BAR
  BAZ
}

type Query {
  foo: Foo @http(path: "/foo")
}
```
````

### Instruction

A header (`---`), followed by an instruction:

```md
---
expect_validation_error: true
---
```

- This instructs the runner to expect a failure when parsing the `server` block and to compare the result with an `errors` snapshot. This is used when testing for error handling.

```md
---
identity: true
---
```

- This instructs the runner to run identity checks on `server` blocks. While it would be good to run this on every test, the code of `server` blocks must be written with this instruction mind, therefore it is optional.

There must be precisely zero or one instruction in a test.

## Test process

1. The runner reads all tests, and selects the ones to run based on the following:
   - If a path to a test was given in the first command line argument, solely that test will run.
   - If one or more tests have a [`skip` annotation](#annotation), every test except those will run.
   - If none of the above is true, all tests will run.
2. The runner evaluates every test.
   1. If the test has an [`SDL error` instruction](#instruction), the runner does the following:
      1. Reads and parses the config, taking note of the validation errors.
      2. **If no validation errors occurred, the runner throws an error.** (`SDL error` is a requirement, not a try-catch.)
      3. Compares the encountered errors to the `errors` snapshot.
      4. If the snapshot doesn't match the encountered errors, the runner generates a new snapshot and throws an error.
      5. Ends the test run, and starts evaluating the next test. (All other actions would require a parseable `@server` block.)
      6. The runner parses every `@server` block.
   2. Parses the block and checks for errors.
   3. If the test has a [`check identity` instruction](#instruction), the runner converts the parsed block to SDL again, and checks if the two strings are the same. If they're not, the runner throws an error.
   4. The runner performs a `merge` check:
      1. Attempts to merge all [`@server` blocks](#server), resulting in a merged config. (If there is a single [`@server` block](#server), the runner will merge it with the default config.)
      2. Compares the merged config to the `merged` snapshot.
      3. If the snapshot doesn't match the merged config, the runner generates a new snapshot and throws an error.
   5. If there is precisely one [`@server` block](#server), the runner performs a `client` check:
      1. Generates the client schema of the `server` block.
      2. Compares it to the `client` SDL snapshot.
      3. If the snapshot doesn't match the generated schema, the runner generates a new snapshot and throws an error.
   6. If the test has an [`@test` block](#test), the runner performs `test` checks:
      1. If there is a [`@mock` block](#mock), the runner sets up the mock HTTP client based on it.
      2. If there is at least one [`@file` block](#filefilename), the runner sets up the mock filesystem based on them.
      3. If there is an [`@env` block](#env), the runner uses it for the app context.
      4. Creates an app context based on the [`@server` block](#server).
      5. For each test in the block (0-based index `i`), the runner does the following:
         1. Runs the HTTP request on the app context.
         2. Compares the HTTP response to the `test_{i}` snapshot.
         3. If the snapshot doesn't match the response, the runner generates a new snapshot and throws an error.

## Snapshots

`execution_spec` uses the [Insta](https://insta.rs) snapshot engine. Snapshots are automatically generated with a `.new` suffix if there is no pre-existing snapshot, or if the compared data didn't match the existing snapshot.

Instead of writing result cases in tests and updating them when behaviour changes, a snapshot-based testing workflow relies on auto-generation. Whenever a `.new` snapshot is generated, it means one of the following:

- Your code made an unexpected breaking change, and you need to fix it.
- Your code made an expected breaking change, and you need to accept the new snapshot.

You need to determine which one is the case, and take action accordingly.

Usage of [cargo-insta](https://insta.rs/docs/cli/) is recommended:

```bash
cargo insta test --review
```

This will regenerate all snapshots without interrupting the test every time there's a diff, and it will also open the snapshot review interface, so that you can accept or reject `.new` snapshots.

## Maintenance

1. To clean unused snapshots, run `cargo insta test --delete-unreferenced-snapshots`.
